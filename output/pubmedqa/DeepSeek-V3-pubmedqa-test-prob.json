[
    {
        "realidx": "10331115",
        "question": "All currently available atypical antipsychotics have, at clinically relevant doses: i) high serotonin (5-HT)2 occupancy; ii) greater 5-HT2 than dopamine (D)2 occupancy; and iii) a higher incidence of extrapyramidal side effects when their D2 occupancy exceeds 80%. A review of pharmacologic and behavioral data suggested that amoxapine should also conform to this profile; therefore, we undertook a positron-emission tomography (PET) study of its 5-HT2 and D2 occupancy.\nSeven healthy volunteers received 50-250 mg/day of amoxapine for 5 days and then had [11C]-raclopride and [18F]-setoperone PET scans.\n5-HT2 receptors showed near saturation at doses of 100 mg/day and above. The D2 receptor occupancies showed a dose-dependent increase, never exceeding 80%; at all doses 5-HT2 occupancy exceeded D2 occupancy.\nIs amoxapine an atypical antipsychotic?",
        "answer": "yes",
        "answer_rationale": "PET data show that amoxapine's profile is very similar to that of the established atypical antipsychotics. These data, together with amoxapine's in vitro pharmacologic profile, effectiveness in animal models, and efficacy in psychotic depression raise the possibility of amoxapine as an \"atypical\" antipsychotic agent in the treatment of schizophrenia.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "All currently available atypical antipsychotics have, at clinically relevant doses: i) high serotonin (5-HT)2 occupancy; ii) greater 5-HT2 than dopamine (D)2 occupancy; and iii) a higher incidence of extrapyramidal side effects when their D2 occupancy exceeds 80%. A review of pharmacologic and behavioral data suggested that amoxapine should also conform to this profile; therefore, we undertook a positron-emission tomography (PET) study of its 5-HT",
        "q2": "2 and D2 occupancy.\nSeven healthy volunteers received 50-250 mg/day of amoxapine for 5 days and then had [11C]-raclopride and [18F]-setoperone PET scans.\n5-HT2 receptors showed near saturation at doses of 100 mg/day and above. The D2 receptor occupancies showed a dose-dependent increase, never exceeding 80%; at all doses 5-HT2 occupancy exceeded D2 occupancy.\nIs amoxapine an atypical antipsychotic?",
        "generated_text": "2 and D2 receptor occupancy in humans. Six healthy volunteers underwent PET scanning with [18F]setoperone and [11C]raclopride at baseline and after 7 days of amoxapine (200 mg/day). Amoxapine's 5-HT2 and D2 receptor occupancies were 88 +/- 6% and 50 +/- 10%, respectively. These results are consistent with the hypothesis that high 5-HT2 occupancy, greater 5-HT2 than D2 occupancy, and D2",
        "levenshtein_distance": 280
    },
    {
        "realidx": "10749257",
        "question": "The United States Food and Drug Administration implemented federal regulations governing mammography under the Mammography Quality Standards Act (MQSA) of 1992. During 1995, its first year in implementation, we examined the impact of the MQSA on the quality of mammography in North Carolina.\nAll mammography facilities were inspected during 1993-1994, and again in 1995. Both inspections evaluated mean glandular radiation dose, phantom image evaluation, darkroom fog, and developer temperature. Two mammography health specialists employed by the North Carolina Division of Radiation Protection performed all inspections and collected and codified data.\nThe percentage of facilities that met quality standards increased from the first inspection to the second inspection. Phantom scores passing rate was 31.6% versus 78.2%; darkroom fog passing rate was 74.3% versus 88.5%; and temperature difference passing rate was 62.4% versus 86.9%.\nHas the mammography quality standards act affected the mammography quality in North Carolina?",
        "answer": "yes",
        "answer_rationale": "In 1995, the first year that the MQSA was in effect, there was a significant improvement in the quality of mammography in North Carolina. This improvement probably resulted from facilities' compliance with federal regulations.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "The United States Food and Drug Administration implemented federal regulations governing mammography under the Mammography Quality Standards Act (MQSA) of 1992. During 1995, its first year in implementation, we examined the impact of the MQSA on the quality of mammography in North Carolina.\nAll mammography facilities were inspected during 1993-1994, and again in 1995. Both inspections evaluated mean glandular radiation dose, phantom image evaluation, darkroom fog, and developer temperature. Two mammography health",
        "q2": " specialists employed by the North Carolina Division of Radiation Protection performed all inspections and collected and codified data.\nThe percentage of facilities that met quality standards increased from the first inspection to the second inspection. Phantom scores passing rate was 31.6% versus 78.2%; darkroom fog passing rate was 74.3% versus 88.5%; and temperature difference passing rate was 62.4% versus 86.9%.\nHas the mammography quality standards act affected the mammography quality in North Carolina?",
        "generated_text": "physicists performed all inspections. The 1995 inspection also evaluated compliance with the MQSA.\nThe mean glandular radiation dose decreased from 1.6 mGy in 1993-1994 to 1.4 mGy in 1995 (P < 0.001). The phantom image evaluation score increased from 3.1 in 1993-1994 to 3.4 in 1995 (P < 0.001). The darkroom fog decreased from 0.10",
        "levenshtein_distance": 383
    },
    {
        "realidx": "10798511",
        "question": "Physical examination to detect abdominal injuries has been considered unreliable in alcohol-intoxicated trauma patients. Computed tomography (CT) plays the primary role in these abdominal evaluations.\nWe reviewed medical records of all blunt trauma patients admitted to our trauma service from January 1, 1992, to March 31, 1998. Study patients had a blood alcohol level>or =80 mg/dL, Glasgow Coma Scale (GCS) score of 15, and unremarkable abdominal examination.\nOf 324 patients studied, 317 (98%) had CT scans negative for abdominal injury. Abdominal injuries were identified in 7 patients (2%), with only 2 (0.6%) requiring abdominal exploration. A significant association was found between major chest injury and abdominal injury.\nBlunt trauma in intoxicated patients: is computed tomography of the abdomen always necessary?",
        "answer": "no",
        "answer_rationale": "The incidence of abdominal injury in intoxicated, hemodynamically stable, blunt trauma patients with a normal abdominal examination and normal mentation is low. Physical examination and attention to clinical risk factors allow accurate abdominal evaluation without CT.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Physical examination to detect abdominal injuries has been considered unreliable in alcohol-intoxicated trauma patients. Computed tomography (CT) plays the primary role in these abdominal evaluations.\nWe reviewed medical records of all blunt trauma patients admitted to our trauma service from January 1, 1992, to March 31, 1998. Study patients had a blood alcohol level>or =80 mg/dL, Glasgow Coma Scale (GCS",
        "q2": ") score of 15, and unremarkable abdominal examination.\nOf 324 patients studied, 317 (98%) had CT scans negative for abdominal injury. Abdominal injuries were identified in 7 patients (2%), with only 2 (0.6%) requiring abdominal exploration. A significant association was found between major chest injury and abdominal injury.\nBlunt trauma in intoxicated patients: is computed tomography of the abdomen always necessary?",
        "generated_text": ") score>or =13, and abdominal CT scan performed. We recorded the results of the initial physical examination, CT scan, and any subsequent laparotomy.\nThere were 1,",
        "levenshtein_distance": 314
    },
    {
        "realidx": "11079675",
        "question": "The purpose of this study is to evaluate right ventricular (RV) volume and function after pulmonary valve replacement (PVR) and to address the issue of optimal surgical timing in these patients.\nChronic pulmonary regurgitation (PR) following repair of tetralogy of Fallot (TOF) leads to RV dilation and an increased incidence of sudden cardiac death in adult patients.\nWe studied 25 consecutive adult patients who underwent PVR for significant PR late after repair of TOF. Radionuclide angiography was performed in all at a mean of 8.2 months (+/- 8 months) before PVR and repeated at a mean of 28.0 months (+/- 22.8 months) after the operation. Right ventricular (RV) end-systolic volume (RVESV), RV end-diastolic volume (RVEDV) and RV ejection fraction (RVEF) were measured.\nMean RVEDV, RVESV and RVEF remained unchanged after PVR (227.1 ml versus 214.9 ml, p = 0.74; 157.4 ml versus 155.4 ml, p = 0.94; 35.6% versus 34.7%, p = 0.78, respectively). Of the 10 patients with RVEF>or = 0.40 before PVR, 5 patients (50%) maintained a RVEF>or = 0.40 following PVR, whereas only 2 out of 15 patients (13%) with pre-operative values<0.40 reached an RVEF>or = 0.40 postoperatively (p<0.001).\nPulmonary valve replacement in adults late after repair of tetralogy of fallot: are we operating too late?",
        "answer": "yes",
        "answer_rationale": "Right ventricular recovery following PVR for chronic significant pulmonary regurgitation after repair of TOF may be compromised in the adult population. In order to maintain adequate RV contractility, pulmonary valve implant in these patients should be considered before RV function deteriorates.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "The purpose of this study is to evaluate right ventricular (RV) volume and function after pulmonary valve replacement (PVR) and to address the issue of optimal surgical timing in these patients.\nChronic pulmonary regurgitation (PR) following repair of tetralogy of Fallot (TOF) leads to RV dilation and an increased incidence of sudden cardiac death in adult patients.\nWe studied 25 consecutive adult patients who underwent PVR for significant PR late after repair of TOF. Radionuclide angiography was performed in all at a mean of 8.2 months (+/- 8 months) before PVR and repeated at a mean of 28.0 months (+/- 22.8 months) after the operation. Right ventricular (RV) end-systolic volume (RVESV), RV end-diastolic volume (RVEDV) and RV ejection fraction (RVEF) were",
        "q2": " measured.\nMean RVEDV, RVESV and RVEF remained unchanged after PVR (227.1 ml versus 214.9 ml, p = 0.74; 157.4 ml versus 155.4 ml, p = 0.94; 35.6% versus 34.7%, p = 0.78, respectively). Of the 10 patients with RVEF>or = 0.40 before PVR, 5 patients (50%) maintained a RVEF>or = 0.40 following PVR, whereas only 2 out of 15 patients (13%) with pre-operative values<0.40 reached an RVEF>or = 0.40 postoperatively (p<0.001).\nPulmonary valve replacement in adults late after repair of tetralogy of fallot: are we operating too late?",
        "generated_text": "measured. The patients were divided into two groups according to the preoperative RVEF: group I (RVEF > or = 40%, n = 12) and group II (RVEF < 40%, n = 13).\nIn the entire group, RVESV and RVEDV decreased significantly after PVR (p < 0.001), but RVEF did not change significantly. In group I, RVESV and RVEDV decreased significantly (p < 0.001), and RVEF increased significantly (p < 0.05). In group II, RVESV and RVEDV decreased significantly (p < 0.001), but RVEF did not change significantly. The RVEF after PVR was significantly higher in group I than in group II (p < 0.01).\nRight ventricular volume decreases significantly after PVR in adult patients with chronic PR late after repair",
        "levenshtein_distance": 533
    },
    {
        "realidx": "11146778",
        "question": "The APACHE II (Acute Physiology and Chronic Health Evaluation II) score used as an intensive care unit (ICU) admission score in emergency surgical patients is not independent of the effects of treatment and might lead to considerable bias in the comparability of defined groups of patients and in the evaluation of treatment policies. Postoperative monitoring with the APACHE II score is clinically irrelevant.\nInception cohort study.\nSecondary referral center.\nEighty-five consecutive emergency surgical patients admitted to the surgical ICU in 1999. The APACHE II score was calculated before surgery; after admission to the ICU; and on postoperative days 3, 7, and 10.\nAPACHE II scores and predicted and observed mortality rates.\nThe mean +/- SD APACHE II score of 24.2 +/- 8.3 at admission to the ICU was approximately 36% greater than the initial APACHE II score of 17.8 +/- 7.7, a difference that was highly statistically significant (P<.001). The overall mortality of 32% favorably corresponds with the predicted mortality of 34% according to the initial APACHE II score. However, the predicted mortality of 50% according to the APACHE II score at admission to the ICU was significantly different from the observed mortality rate (P =.02). In 40 long-term patients (>/=10 days in the ICU), the difference between the APACHE II scores of survivors and patients who died was statistically significant on day 10 (P =.04).\nRisk stratification in emergency surgical patients: is the APACHE II score a reliable marker of physiological impairment?",
        "answer": "yes",
        "answer_rationale": "For risk stratification in emergency surgical patients, it is essential to measure the APACHE II score before surgical treatment. Longitudinal APACHE II scoring reveals continuous improvement of the score in surviving patients but has no therapeutic relevance in the individual patient.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "The APACHE II (Acute Physiology and Chronic Health Evaluation II) score used as an intensive care unit (ICU) admission score in emergency surgical patients is not independent of the effects of treatment and might lead to considerable bias in the comparability of defined groups of patients and in the evaluation of treatment policies. Postoperative monitoring with the APACHE II score is clinically irrelevant.\nInception cohort study.\nSecondary referral center.\nEighty-five consecutive emergency surgical patients admitted to the surgical ICU in 1999. The APACHE II score was calculated before surgery; after admission to the ICU; and on postoperative days 3, 7, and 10.\nAPACHE II scores and predicted and observed mortality rates.\nThe mean +/- SD APACHE II score of 24.2 +/- 8.3 at admission to",
        "q2": " the ICU was approximately 36% greater than the initial APACHE II score of 17.8 +/- 7.7, a difference that was highly statistically significant (P<.001). The overall mortality of 32% favorably corresponds with the predicted mortality of 34% according to the initial APACHE II score. However, the predicted mortality of 50% according to the APACHE II score at admission to the ICU was significantly different from the observed mortality rate (P =.02). In 40 long-term patients (>/=10 days in the ICU), the difference between the APACHE II scores of survivors and patients who died was statistically significant on day 10 (P =.04).\nRisk stratification in emergency surgical patients: is the APACHE II score a reliable marker of physiological impairment?",
        "generated_text": "the ICU was significantly higher than the preoperative score of 20.1 +/- 7.3 (P<.001). The predicted mortality rate of 48.5% was significantly higher than the observed mortality rate of 20.0% (P<.001). The APACHE II score decreased significantly from 24.2 +/- 8.3 at admission to 15. 0 +/- 6.3 on postoperative day 3 (P<.001). The predicted mortality rate decreased from 48.5% at admission to 20.0% on postoperative day 3 (P<.001). The observed mortality rate was 20.0% at admission and 0% on postoperative day 3 (P<.001).\nThe APACHE II score is not independent of the effects",
        "levenshtein_distance": 507
    },
    {
        "realidx": "11458136",
        "question": "By requiring or encouraging enrollees to obtain a usual source of care, managed care programs hope to improve access to care without incurring higher costs.\n(1) To examine the effects of managed care on the likelihood of low-income persons having a usual source of care and a usual physician, and; (2) To examine the association between usual source of care and access.\nCross-sectional survey of households conducted during 1996 and 1997.\nA nationally representative sample of 14,271 low-income persons.\nUsual source of care, usual physician, managed care enrollment, managed care penetration.\nHigh managed care penetration in the community is associated with a lower likelihood of having a usual source of care for uninsured persons (54.8% vs. 62.2% in low penetration areas) as well as a lower likelihood of having a usual physician (60% vs. 72.8%). Managed care has only marginal effects on the likelihood of having a usual source of care for privately insured and Medicaid beneficiaries. Having a usual physician substantially reduces unmet medical needs for the insured but less so for the uninsured.\nDoes managed care enable more low income persons to identify a usual source of care?",
        "answer": "maybe",
        "answer_rationale": "Having a usual physician can be an effective tool in improving access to care for low-income populations, although it is most effective when combined with insurance coverage. However, the effectiveness of managed care in linking more low-income persons to a medical home is uncertain, and may have unintended consequences for uninsured persons.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "By requiring or encouraging enrollees to obtain a usual source of care, managed care programs hope to improve access to care without incurring higher costs.\n(1) To examine the effects of managed care on the likelihood of low-income persons having a usual source of care and a usual physician, and; (2) To examine the association between usual source of care and access.\nCross-sectional survey of households conducted during 1996 and 1997.\nA nationally representative sample of 14,271 low-income persons.\nUsual source of care, usual physician, managed care enrollment, managed care penetration",
        "q2": ".\nHigh managed care penetration in the community is associated with a lower likelihood of having a usual source of care for uninsured persons (54.8% vs. 62.2% in low penetration areas) as well as a lower likelihood of having a usual physician (60% vs. 72.8%). Managed care has only marginal effects on the likelihood of having a usual source of care for privately insured and Medicaid beneficiaries. Having a usual physician substantially reduces unmet medical needs for the insured but less so for the uninsured.\nDoes managed care enable more low income persons to identify a usual source of care?",
        "generated_text": ", and access to care.\nLow-income persons enrolled in managed care were more likely to have a usual source of care and a usual physician than those with fee-for-service coverage. Managed care enrollees in areas with high managed care penetration were more likely to have a usual source of care and a usual physician than those in areas with low managed care penetration. Having a usual source of care was associated with better access to care, but managed care enrollees with a usual source of care were less likely to report good access than fee-for-service enrollees with a usual source of care.\nManaged care increases the likelihood",
        "levenshtein_distance": 457
    },
    {
        "realidx": "11567820",
        "question": "To test the hypothesis that increasing the nerve length within the treatment volume for trigeminal neuralgia radiosurgery would improve pain relief.\nEighty-seven patients with typical trigeminal neuralgia were randomized to undergo retrogasserian gamma knife radiosurgery (75 Gy maximal dose with 4-mm diameter collimators) using either one (n = 44) or two (n = 43) isocenters. The median follow-up was 26 months (range 1-36).\nPain relief was complete in 57 patients (45 without medication and 12 with low-dose medication), partial in 15, and minimal in another 15 patients. The actuarial rate of obtaining complete pain relief (with or without medication) was 67.7% +/- 5.1%. The pain relief was identical for one- and two-isocenter radiosurgery. Pain relapsed in 30 of 72 responding patients. Facial numbness and mild and severe paresthesias developed in 8, 5, and 1 two-isocenter patients vs. 3, 4, and 0 one-isocenter patients, respectively (p = 0.23). Improved pain relief correlated with younger age (p = 0.025) and fewer prior procedures (p = 0.039) and complications (numbness or paresthesias) correlated with the nerve length irradiated (p = 0.018).\nDoes increased nerve length within the treatment volume improve trigeminal neuralgia radiosurgery?",
        "answer": "no",
        "answer_rationale": "Increasing the treatment volume to include a longer nerve length for trigeminal neuralgia radiosurgery does not significantly improve pain relief but may increase complications.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "To test the hypothesis that increasing the nerve length within the treatment volume for trigeminal neuralgia radiosurgery would improve pain relief.\nEighty-seven patients with typical trigeminal neuralgia were randomized to undergo retrogasserian gamma knife radiosurgery (75 Gy maximal dose with 4-mm diameter collimators) using either one (n = 44) or two (n = 43) isocenters. The median follow-up was 26 months (range 1-36).\nPain relief was complete in 57 patients (45 without medication and 12 with low-dose medication), partial in 15, and minimal in another 15 patients. The actuarial rate of obtaining complete pain relief (with or without medication) was 67.7% +/- ",
        "q2": "5.1%. The pain relief was identical for one- and two-isocenter radiosurgery. Pain relapsed in 30 of 72 responding patients. Facial numbness and mild and severe paresthesias developed in 8, 5, and 1 two-isocenter patients vs. 3, 4, and 0 one-isocenter patients, respectively (p = 0.23). Improved pain relief correlated with younger age (p = 0.025) and fewer prior procedures (p = 0.039) and complications (numbness or paresthesias) correlated with the nerve length irradiated (p = 0.018).\nDoes increased nerve length within the treatment volume improve trigeminal neuralgia radiosurgery?",
        "generated_text": "5.1% at 1 year and 75.1% +/- 5.1% at 2 years. The median time to pain relief was 1 month (range 1 d to 6 mo). The actuarial rate of maintaining complete pain relief (with or without medication) was 77.1% +/- 5.1% at 1 year and 68.5% +/- 6.1% at 2 years. The median time to pain recurrence was 12 months (range 3-24 mo). The actuarial rate of developing new facial sensory dysfunction was 45.7% +/- 5.4% at 1 year and 54.3% +/- 5.6% at 2 years. The",
        "levenshtein_distance": 439
    },
    {
        "realidx": "11955750",
        "question": "To show the results of treating posterior uveal melanomas with 106Ru plaque beta-ray radiotherapy and to review and discuss the literature concerning the optimal apical dose prescription (100 vs. 160 Gy).\nForty-eight patients with uveal melanomas (median height 3.85 mm + 1 mm sclera) were treated with ruthenium plaques. The median apical dose was 120 Gy, the median scleral dose 546 Gy.\nAfter 5.8 years of follow-up, the overall 5-year survival rate was 90%, the disease specific 5-year survival rate was 92% (3 patients alive with metastasis). Six percent received a second ruthenium application, 10% of the eyes had to be enucleated. Local control was achieved in 90% of the patients with conservative therapy alone. Central or paracentral tumors showed 50% of the pretherapeutic vision after 4 years, and 80% of the vision was preserved in those with peripheral tumors. The main side effects were mostly an uncomplicated retinopathy (30%); macular degeneration or scarring led to poor central vision in 30% of cases.\nDoes escalation of the apical dose change treatment outcome in beta-radiation of posterior choroidal melanomas with 106Ru plaques?",
        "answer": "no",
        "answer_rationale": "Brachytherapy with ruthenium applicators is an effective therapy for small- and medium-size posterior uveal melanomas. Our results are comparable to other series. The treatment outcome does not seem to be capable of improvement by increasing the apical dose. An internationally accepted model for defining the dosage in brachytherapy is needed.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "To show the results of treating posterior uveal melanomas with 106Ru plaque beta-ray radiotherapy and to review and discuss the literature concerning the optimal apical dose prescription (100 vs. 160 Gy).\nForty-eight patients with uveal melanomas (median height 3.85 mm + 1 mm sclera) were treated with ruthenium plaques. The median apical dose was 120 Gy, the median scleral dose 546 Gy.\nAfter 5.8 years of follow-up, the overall 5-year survival rate was 90%, the disease specific 5-year survival rate was 92% (3 patients alive with metastasis). Six",
        "q2": " percent received a second ruthenium application, 10% of the eyes had to be enucleated. Local control was achieved in 90% of the patients with conservative therapy alone. Central or paracentral tumors showed 50% of the pretherapeutic vision after 4 years, and 80% of the vision was preserved in those with peripheral tumors. The main side effects were mostly an uncomplicated retinopathy (30%); macular degeneration or scarring led to poor central vision in 30% of cases.\nDoes escalation of the apical dose change treatment outcome in beta-radiation of posterior choroidal melanomas with 106Ru plaques?",
        "generated_text": "patients developed local recurrences (12.5%). The 5-year enucleation rate was 8.3%. The 5-year visual acuity was 0.5 or better in 50% of the patients. The 5-year rate of radiation induced complications was 25% for maculopathy, 12.5% for papillopathy, 8.3% for rubeosis iridis and 4.2% for neovascular glaucoma.\nThe results of this study are comparable to those of other studies using 106Ru plaques. The local recurrence rate of 12.5% is higher than in other studies, but the follow-up period is longer",
        "levenshtein_distance": 439
    },
    {
        "realidx": "12377809",
        "question": "Dyschesia can be provoked by inappropriate defecation movements. The aim of this prospective study was to demonstrate dysfunction of the anal sphincter and/or the musculus (m.) puborectalis in patients with dyschesia using anorectal endosonography.\nTwenty consecutive patients with a medical history of dyschesia and a control group of 20 healthy subjects underwent linear anorectal endosonography (Toshiba models IUV 5060 and PVL-625 RT). In both groups, the dimensions of the anal sphincter and the m. puborectalis were measured at rest, and during voluntary squeezing and straining. Statistical analysis was performed within and between the two groups.\nThe anal sphincter became paradoxically shorter and/or thicker during straining (versus the resting state) in 85% of patients but in only 35% of control subjects. Changes in sphincter length were statistically significantly different (p<0.01, chi(2) test) in patients compared with control subjects. The m. puborectalis became paradoxically shorter and/or thicker during straining in 80% of patients but in only 30% of controls. Both the changes in length and thickness of the m. puborectalis were significantly different (p<0.01, chi(2) test) in patients versus control subjects.\nIs anorectal endosonography valuable in dyschesia?",
        "answer": "yes",
        "answer_rationale": "Linear anorectal endosonography demonstrated incomplete or even absent relaxation of the anal sphincter and the m. puborectalis during a defecation movement in the majority of our patients with dyschesia. This study highlights the value of this elegant ultrasonographic technique in the diagnosis of \"pelvic floor dyssynergia\" or \"anismus\".",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Dyschesia can be provoked by inappropriate defecation movements. The aim of this prospective study was to demonstrate dysfunction of the anal sphincter and/or the musculus (m.) puborectalis in patients with dyschesia using anorectal endosonography.\nTwenty consecutive patients with a medical history of dyschesia and a control group of 20 healthy subjects underwent linear anorectal endosonography (Toshiba models IUV 5060 and PVL-625 RT). In both groups, the dimensions of the anal sphincter and the m. puborectalis were measured at rest, and during voluntary squeezing and straining. Statistical analysis was performed within and between the two groups.\nThe anal",
        "q2": " sphincter became paradoxically shorter and/or thicker during straining (versus the resting state) in 85% of patients but in only 35% of control subjects. Changes in sphincter length were statistically significantly different (p<0.01, chi(2) test) in patients compared with control subjects. The m. puborectalis became paradoxically shorter and/or thicker during straining in 80% of patients but in only 30% of controls. Both the changes in length and thickness of the m. puborectalis were significantly different (p<0.01, chi(2) test) in patients versus control subjects.\nIs anorectal endosonography valuable in dyschesia?",
        "generated_text": "sphincter and the m. puborectalis were significantly thicker in patients with dyschesia than in the control group (P < 0.05). In the control group, the anal sphincter and the m. puborectalis were significantly thicker during voluntary squeezing than at rest (P < 0.05). In the patient group, the anal sphincter and the m. puborectalis were significantly thicker during voluntary squeezing and straining than at rest (P < 0.05). In the patient group, the anal sphincter and the m. puborectalis were significantly thicker during straining than during voluntary squeezing (P < 0.05). In the control group, the anal sph",
        "levenshtein_distance": 415
    },
    {
        "realidx": "12419743",
        "question": "To determine whether patients with high-risk metastatic breast cancer draw benefit from combination chemotherapy as first-line treatment.\nA total of 260 women with measurable metastatic breast cancer fulfilling high-risk criteria, previously untreated with chemotherapy for their metastatic disease, were randomized to receive either mitoxantrone 12 mg/m(2) or the combination of fluorouracil 500 mg/m(2), epirubicin 50 mg/m(2) and cyclophosphamide 500 mg/m(2) (FEC) every 3 weeks. Treatment was continued until complete remission plus two cycles, or until disease progression. In the case of partial remission or stable disease, treatment was stopped after 12 cycles. Second-line treatment was vindesine, mitomycin and prednisolone. Gain from treatment was estimated using a modified Brunner's score composed of time to progression, patients' rating of the treatment benefit, alopecia, vomiting and performance status.\nAfter recruitment from 1992 to 1997 and observation from 1997 to 1999, the final evaluation showed that single-agent treatment with mitoxantrone does not differ significantly from combination treatment with FEC in terms of response, objective remission rate, remission duration, time to response, time to best response, time to progression or overall survival. There was, however, a significant difference in gain from treatment using a modified Brunner's score favoring the single-agent treatment arm. There was no evidence that any subgroup would fare better with combination treatment.\nIs first-line single-agent mitoxantrone in the treatment of high-risk metastatic breast cancer patients as effective as combination chemotherapy?",
        "answer": "yes",
        "answer_rationale": "No significant difference was detected between the treatment with mitoxantrone as a single agent and the combination of low-dose FEC in terms of response or survival; therefore, the imperative of the necessity of first-line combination chemotherapy for patients with high-risk metastatic breast cancer may be questioned. Since toxicity and quality of life score favored the single-agent mitoxantrone treatment arm, this treatment may be offered to patients preferring quality of life to a potential small prolongation of survival.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To determine whether patients with high-risk metastatic breast cancer draw benefit from combination chemotherapy as first-line treatment.\nA total of 260 women with measurable metastatic breast cancer fulfilling high-risk criteria, previously untreated with chemotherapy for their metastatic disease, were randomized to receive either mitoxantrone 12 mg/m(2) or the combination of fluorouracil 500 mg/m(2), epirubicin 50 mg/m(2) and cyclophosphamide 500 mg/m(2) (FEC) every 3 weeks. Treatment was continued until complete remission plus two cycles, or until disease progression. In the case of partial remission or stable disease, treatment was stopped after 12 cycles. Second-line treatment was vindesine, mitomycin and prednisolone. Gain from treatment was estimated using a modified",
        "q2": " Brunner's score composed of time to progression, patients' rating of the treatment benefit, alopecia, vomiting and performance status.\nAfter recruitment from 1992 to 1997 and observation from 1997 to 1999, the final evaluation showed that single-agent treatment with mitoxantrone does not differ significantly from combination treatment with FEC in terms of response, objective remission rate, remission duration, time to response, time to best response, time to progression or overall survival. There was, however, a significant difference in gain from treatment using a modified Brunner's score favoring the single-agent treatment arm. There was no evidence that any subgroup would fare better with combination treatment.\nIs first-line single-agent mitoxantrone in the treatment of high-risk metastatic breast cancer patients as effective as combination chemotherapy?",
        "generated_text": "Brunner score, which combines the effects of treatment on response, time to progression and survival.\nThe overall response rate was 30% for mitoxantrone and 50% for FEC (P = 0.001). The median time to progression was 4.5 months for mitoxantrone and 7.5 months for FEC (P = 0.0001). The median survival was 10.5 months for mitoxantrone and 14.5 months for FEC (P = 0.0001). The Brunner score was 0.33 for mitoxantrone and 0.66 for FEC (P = 0.0001). The toxicity of FEC was higher than that of mitoxantrone, but was manageable.\nPatients with high",
        "levenshtein_distance": 618
    },
    {
        "realidx": "12442934",
        "question": "To determine whether prior exposure of non-steroidal anti-inflammatory drugs increases perioperative blood loss associated with major orthopaedic surgery.\nFifty patients scheduled for total hip replacement were allocated to two groups (double blind, randomized manner). All patients were pretreated for 2 weeks before surgery: Group 1 with placebo drug, Group 2 with ibuprofen. All patients were injected intrathecally with bupivacaine 20mg plus morphine 0.1 mg, in a total volume of 4 mL, to provide surgical anaesthesia.\nThe presence of severe adverse effects caused eight patients in the ibuprofen group and six in the placebo group to terminate their participation in the trial. The perioperative blood loss increased by 45% in the ibuprofen group compared with placebo. The total (+/-SD) blood loss in the ibuprofen group was 1161 (+/-472) mL versus 796 (+/-337) mL in the placebo group.\nDoes ibuprofen increase perioperative blood loss during hip arthroplasty?",
        "answer": "yes",
        "answer_rationale": "Pretreatment with ibuprofen before elective total hip surgery increases the perioperative blood loss significantly. Early discontinuation of non-selective non-steroidal anti-inflammatory drugs is advised.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To determine whether prior exposure of non-steroidal anti-inflammatory drugs increases perioperative blood loss associated with major orthopaedic surgery.\nFifty patients scheduled for total hip replacement were allocated to two groups (double blind, randomized manner). All patients were pretreated for 2 weeks before surgery: Group 1 with placebo drug, Group 2 with ibuprofen. All patients were injected intrathecally with bupivacaine 20mg plus morphine 0.1 mg, in a total volume of 4 m",
        "q2": "L, to provide surgical anaesthesia.\nThe presence of severe adverse effects caused eight patients in the ibuprofen group and six in the placebo group to terminate their participation in the trial. The perioperative blood loss increased by 45% in the ibuprofen group compared with placebo. The total (+/-SD) blood loss in the ibuprofen group was 1161 (+/-472) mL versus 796 (+/-337) mL in the placebo group.\nDoes ibuprofen increase perioperative blood loss during hip arthroplasty?",
        "generated_text": "ls. The following parameters were recorded: blood loss during surgery, blood loss in the drains, total blood loss, blood transfused, haemoglobin and haematocrit levels preoperatively and on the first and third postoperative days, and the need for blood transfusion.\nThe mean total blood loss was 1,500 ml in the placebo group and 1,600 ml in the ibuprofen group. The mean blood loss in the drains was 1,000 ml in the placebo group and 1,100 ml in the ibuprofen",
        "levenshtein_distance": 360
    },
    {
        "realidx": "12769830",
        "question": "Most staging systems for soft tissue sarcoma are based on histologic malignancy-grade, tumor size and tumor depth. These factors are generally dichotomized, size at 5 cm. We believe it is unlikely that tumor depth per se should influence a tumor's metastatic capability. Therefore we hypothesized that the unfavourable prognostic importance of depth could be explained by the close association between size and depth, deep-seated tumors on average being larger than the superficial ones. When tumor size is dichotomized, this effect should be most pronounced in the large size (>5 cm) group in which the size span is larger.\nWe analyzed the associations between tumor size and depth and the prognostic importance of grade, size and depth in a population-based series of 490 adult patients with soft tissue sarcoma of the extremity or trunk wall with complete, 4.5 years minimum, follow-up.\nMultivariate analysis showed no major prognostic effect of tumor depth when grade and size were taken into account. The mean size of small tumors was the same whether superficial or deep but the mean size of large and deep-seated tumors were one third larger than that of large but superficial tumors. Tumor depth influenced the prognosis in the subset of high-grade and large tumors. In this subset deep-seated tumors had poorer survival rate than superficial tumors, which could be explained by the larger mean size of the deep-seated tumors.\nShould tumor depth be included in prognostication of soft tissue sarcoma?",
        "answer": "no",
        "answer_rationale": "Most of the prognostic value of tumor depth in soft tissue sarcomas of the extremity or trunk wall can be explained by the association between tumor size and depth.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Most staging systems for soft tissue sarcoma are based on histologic malignancy-grade, tumor size and tumor depth. These factors are generally dichotomized, size at 5 cm. We believe it is unlikely that tumor depth per se should influence a tumor's metastatic capability. Therefore we hypothesized that the unfavourable prognostic importance of depth could be explained by the close association between size and depth, deep-seated tumors on average being larger than the superficial ones. When tumor size is dichotomized, this effect should be most pronounced in the large size (>5 cm) group in which the size span is larger.\nWe analyzed the associations between tumor size and depth and the prognostic importance of grade, size and depth in a population-based",
        "q2": " series of 490 adult patients with soft tissue sarcoma of the extremity or trunk wall with complete, 4.5 years minimum, follow-up.\nMultivariate analysis showed no major prognostic effect of tumor depth when grade and size were taken into account. The mean size of small tumors was the same whether superficial or deep but the mean size of large and deep-seated tumors were one third larger than that of large but superficial tumors. Tumor depth influenced the prognosis in the subset of high-grade and large tumors. In this subset deep-seated tumors had poorer survival rate than superficial tumors, which could be explained by the larger mean size of the deep-seated tumors.\nShould tumor depth be included in prognostication of soft tissue sarcoma?",
        "generated_text": "series of 559 patients with soft tissue sarcoma of the extremity or trunk wall.\nTumor size was significantly associated with depth, deep-seated tumors being larger than the superficial ones (P < 0.001). In the large size group, the deep-seated tumors were significantly larger than the superficial ones (P < 0.001). In the small size group, there was no significant difference in size between deep and superficial tumors. Tumor depth was significantly associated with metastasis-free survival in the large size group (P = 0.001) but not in the small size group (P = 0.6). In the large size group, the prognostic importance of depth was lost when size was considered as a continuous variable (P =",
        "levenshtein_distance": 488
    },
    {
        "realidx": "15053041",
        "question": "Increased aortic stiffness is a independent risk factor of cardiovascular disease in patients with hypertension. Acute changes of the heart rate (HR) have been reported not to affect the aortic stiffness in pacing. However, it is unknown whether acute changes in HR caused by sympathomimetics can affect the aortic stiffness in patients with hypertension. We investigated the effect of acute changes in HR produced by isoproterenol on the aortic stiffness in 17 hypertensive patientss (mean age: 59 +/- 9 years).\nAll vasoactive drugs were discontinued at least 3 days before the study. The carotid-to-femoral pulse wave velocity (PWV) was measured by the foot-to-foot method. The pulse waves were recorded at the baseline and at every increase of HR by 5 to 10 bpm with a gradual increase of the dose of isoproterenol. The blood pressures and HR were measured simultaneously. For the analysis, HR, PWV, compliance (C), and compliance index (Ci) were converted as percent changes (delta) from the baseline values. Percent changes of the parameters of the aortic stiffness, i.e., deltaPWV, deltaC, and deltaCi, were grouped by every 10% increase in deltaHR.\nThere was no significant difference among groups in deltaPWV, deltaC and deltaCi (p>0.05 for each of the group). The regression analysis showed no significant correlation of deltaHR with deltaPWV and deltaC (r = 0.18, 0.13 respectively, p>0.05 for each). deltaCi had a poor correlation with deltaHR (r = 0.22, p<0.05). However, only 4.6% of deltaCi could be referred to deltaHR (r2 = 0.046).\nDo acute changes in heart rate by isoproterenol affect aortic stiffness in patients with hypertension?",
        "answer": "no",
        "answer_rationale": "Aortic stiffness was not affected by acute changes in HR produced by isoproterenol which suggests that it is not necessary to consider acute changes in HR when measuring aortic PWV.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Increased aortic stiffness is a independent risk factor of cardiovascular disease in patients with hypertension. Acute changes of the heart rate (HR) have been reported not to affect the aortic stiffness in pacing. However, it is unknown whether acute changes in HR caused by sympathomimetics can affect the aortic stiffness in patients with hypertension. We investigated the effect of acute changes in HR produced by isoproterenol on the aortic stiffness in 17 hypertensive patientss (mean age: 59 +/- 9 years).\nAll vasoactive drugs were discontinued at least 3 days before the study. The carotid-to-femoral pulse wave velocity (PWV) was measured by the foot-to-foot method. The pulse waves were recorded at the baseline and at every increase of HR by 5 to 10 bpm with a gradual increase of the dose of isoproterenol. The blood pressures and HR were measured simultaneously. For the analysis, HR, PWV, compliance",
        "q2": " (C), and compliance index (Ci) were converted as percent changes (delta) from the baseline values. Percent changes of the parameters of the aortic stiffness, i.e., deltaPWV, deltaC, and deltaCi, were grouped by every 10% increase in deltaHR.\nThere was no significant difference among groups in deltaPWV, deltaC and deltaCi (p>0.05 for each of the group). The regression analysis showed no significant correlation of deltaHR with deltaPWV and deltaC (r = 0.18, 0.13 respectively, p>0.05 for each). deltaCi had a poor correlation with deltaHR (r = 0.22, p<0.05). However, only 4.6% of deltaCi could be referred to deltaHR (r2 = 0.046).\nDo acute changes in heart rate by isoproterenol affect aortic stiffness in patients with hypertension?",
        "generated_text": ", and distensibility were compared at the baseline and at the peak HR.\nThe baseline HR was 60 +/- 6 bpm and the peak HR was 90 +/- 6 bpm. The baseline PWV was 1, 200 +/- 200 cm/s and the peak PWV was 1, 200 +/- 200 cm/s (p = NS). The baseline compliance was 0. 6 +/- 0. 2 ml/mmHg and the peak compliance was 0. 6 +/- 0. 2 ml/mmHg (p = NS). The baseline distensibility was 0. 002 +/- 0. 001 mmHg(-1) and the peak distensibility was 0. 002 +/- 0. 001 mmHg(-1) (p = NS). The baseline systolic blood pressure was 150 +/- 20 mmHg and the peak systolic blood pressure was 150 +/- 20 mmHg (p = NS).",
        "levenshtein_distance": 557
    },
    {
        "realidx": "15597845",
        "question": "The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to be a potent inhibitor of cell growth and a strong anti-angiogenic substance. We investigated for the first time whether in vitro combinations of 2ME with various chemotherapeutic compounds may result in an additive inhibitory effect on the proliferation of human ovary cancer cells.\nAs a model two different human ovary cancer cell lines were used. All cell lines were incubated with equimolar concentrations of 2ME (0.8-25 microM) and the chemotherapeutics epirubicine, doxorubicine, paclitaxel, docetaxel, carboplatin, vinorelbine, 5-fluorouracil and mafosfamide. Proliferation was measured after four days using the ATP-chemosensitivity test.\nFor both ovary cancer cell lines a significant additive effect of 2ME with epirubicine and carboplatin was observed at the lower concentration range of these chemotherapeutic substances.\nIs the combination with 2-methoxyestradiol able to reduce the dosages of chemotherapeutices in the treatment of human ovarian cancer?",
        "answer": "yes",
        "answer_rationale": "2ME is able to enhance the antiproliferative activity of certain chemotherapeutics at pharmacological relevant concentrations. This estradiol metabolite is currently in a phase II trial in patients with refractary metastatic breast cancer and the tolerability has been shown to be very good. The combination of 2ME with chemotherapeutics may therefore offer a new clinically relevant treatment regimen for hormone-dependent cancer.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to be a potent inhibitor of cell growth and a strong anti-angiogenic substance. We investigated for the first time whether in vitro combinations of 2ME with various chemotherapeutic compounds may result in an additive inhibitory effect on the proliferation of human ovary cancer cells.\nAs a model two different human ovary cancer cell lines were used. All cell lines were incubated with equimolar concentrations of 2ME (0.8-25 microM) and the chemotherapeutics epirub",
        "q2": "icine, doxorubicine, paclitaxel, docetaxel, carboplatin, vinorelbine, 5-fluorouracil and mafosfamide. Proliferation was measured after four days using the ATP-chemosensitivity test.\nFor both ovary cancer cell lines a significant additive effect of 2ME with epirubicine and carboplatin was observed at the lower concentration range of these chemotherapeutic substances.\nIs the combination with 2-methoxyestradiol able to reduce the dosages of chemotherapeutices in the treatment of human ovarian cancer?",
        "generated_text": "icine, paclitaxel, topotecan, and cisplatin (0.8-25 microM) alone and in combination. After 72 h of incubation, the number of viable cells was determined by the MTT assay. The interaction between 2ME and the chemotherapeutic agents was analyzed by the median effect method.\nThe combination of 2ME with epirubicine, paclitaxel, topotecan, and cisplatin resulted in a synergistic antiproliferative effect in both cell lines. The combination index (CI) values were in the range of 0.1-0.9, indicating a strong",
        "levenshtein_distance": 388
    },
    {
        "realidx": "16418930",
        "question": "Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.\n100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.\nDifferences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.\nLandolt C and snellen e acuity: differences in strabismus amblyopia?",
        "answer": "no",
        "answer_rationale": "Using the charts described, there was only a slight overestimation of visual acuity by the Snellen E compared to the Landolt C, even in strabismus amblyopia. Small differences in the lower visual acuity range have to be considered.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.\n100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while",
        "q2": " wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.\nDifferences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.\nLandolt C and snellen e acuity: differences in strabismus amblyopia?",
        "generated_text": "the next line had to be read completely. The difference between the two acuities was calculated and the results were analyzed with the Wilcoxon test.\nThe median visual acuity was 0.3 (range 0.02 - 1.6) for the Snellen E and 0.4 (range 0.02 - 1.6) for the Landolt C. The median difference was 0.1 (range 0 - 0.5). The difference was statistically significant (p < 0.001). The difference was 0.1 in 50 patients, 0.2 in 25 patients, 0.3 in 7 patients, 0.4 in 3 patients, and 0.5 in 1 patient. In 15 patients, the difference was 0. The difference was 0.1",
        "levenshtein_distance": 494
    },
    {
        "realidx": "16538201",
        "question": "To determine whether the use of hydrophilic guidewires has increased the technical success rate of peripheral percutaneous transluminal angioplasty (PTA).MATERIAL/\nWe performed 125 procedures and analyzed the technical success rates of PTA using the conventional guidewire first and then if needed, the hydrophilic guidewire for iliac and SFA stenoses or occlusions. Angioplasty was performed on 25 stenosed, 25 occluded iliac arteries and 25 stenosed, 50 occluded femoral arteries. The result was defined as technical success when the lesion was crossed by a guidewire and balloon, then it was dilated with restoration of vessel lumen and less than 30% residual stenosis and the rise in ABI values was at least 0.15 after 24 hours.\nThe technical success rate after PTA of stenosed iliac arteries was achieved in 96% (24/25) using conventional wires and 100% using hydrophilic guidewire; in iliac occlusions, the rates were 60% (15/25) and 96%, respectively; in femoral stenosis - 84% (21/25) and 100%; in occlusions in the first group: lesions<10 cm -64% (16/25) and 96%, in the second group: lesions>10 cm -48% (12/25) and 88%. In the iliac group, there was no significant difference in the success of iliac stenosis PTA. However, there were significant changes in the success rates of PTA performed for SFA stenosis and iliac and SFA occlusions.\nDoes use of hydrophilic guidewires significantly improve technical success rates of peripheral PTA?",
        "answer": "maybe",
        "answer_rationale": "In summary, we report an overall improvement and high technical success rate for peripherial PTA. The use of hydrophilic guidewires made significant difference to the technical success rate of PTA, especially in occlusion and more complicated lesions.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "To determine whether the use of hydrophilic guidewires has increased the technical success rate of peripheral percutaneous transluminal angioplasty (PTA).MATERIAL/\nWe performed 125 procedures and analyzed the technical success rates of PTA using the conventional guidewire first and then if needed, the hydrophilic guidewire for iliac and SFA stenoses or occlusions. Angioplasty was performed on 25 stenosed, 25 occluded iliac arteries and 25 stenosed, 50 occluded femoral arteries. The result was defined as technical success when the lesion was crossed by a guidewire and balloon, then it was dilated with restoration of vessel lumen and less than 30% residual stenosis and the rise in ABI values was at least 0.15 after 24 hours.\nThe technical success rate after PTA of stenosed iliac arteries",
        "q2": " was achieved in 96% (24/25) using conventional wires and 100% using hydrophilic guidewire; in iliac occlusions, the rates were 60% (15/25) and 96%, respectively; in femoral stenosis - 84% (21/25) and 100%; in occlusions in the first group: lesions<10 cm -64% (16/25) and 96%, in the second group: lesions>10 cm -48% (12/25) and 88%. In the iliac group, there was no significant difference in the success of iliac stenosis PTA. However, there were significant changes in the success rates of PTA performed for SFA stenosis and iliac and SFA occlusions.\nDoes use of hydrophilic guidewires significantly improve technical success rates of peripheral PTA?",
        "generated_text": "was 100% (25/25) with the conventional guidewire, and 100% (25/25) with the hydrophilic guidewire. The technical success rate after PTA of occluded iliac arteries was 60% (15/25) with the conventional guidewire, and 100% (25/25) with the hydrophilic guidewire. The technical success rate after PTA of stenosed femoral arteries was 100% (25/25) with the conventional guidewire, and 100% (25/25) with the hydrophilic guidewire. The technical success rate after PTA of occluded femoral arteries was 60% (30/50) with the conventional guidewire, and 100% (50/50) with the hydrophilic guidewire.\nThe use of hydrophilic guidewires has increased the technical success rate of PTA in occluded ili",
        "levenshtein_distance": 469
    },
    {
        "realidx": "16772913",
        "question": "This study provides the first large-volume (1000 implant) comparison of the deflation rates of Poly Implant Prosthesis prefilled textured saline breast implants versus a control group of Mentor Siltex textured saline implants.\nA consecutive series of 500 Poly Implant Prosthesis prefilled textured saline breast implants was compared with a consecutive series of 500 Mentor Siltex breast implants. Each breast implant was evaluated for a 4-year period, and the annual deflation rate (number of deflations during a given year divided by the total number of implants) and cumulative deflation rate (cumulative total of deflations through a given year divided by the total number of implants) were recorded. Statistical significance was calculated using the Fisher's exact test at year 1 and the chi-square analysis at years 2 through 4.\nThe cumulative deflation rates of the Poly Implant Prosthesis implants was as follows: year 1, 1.2 percent; year 2, 5.6 percent; year 3, 11.4 percent; and year 4, 15.4 percent. The cumulative deflation rates of the Mentor implants was: year 1, 0.2 percent; year 2, 0.6 percent; year 3, 1.6 percent; and year 4, 4.4 percent. At year 1, the difference between deflation rates was not statistically significant (Fisher's exact test, p>0.05). However, at year 2 (chi-square, 13.29; p<0.001), year 3 (chi-square, 37.91; p<0.001), and year 4 (chi-square, 32.69; p<0.001), the difference was statistically significant.\nA comparison of 500 prefilled textured saline breast implants versus 500 standard textured saline breast implants: is there a difference in deflation rates?",
        "answer": "yes",
        "answer_rationale": "There was a statistically significant difference between the overall deflation rates of Poly Implant Prosthesis prefilled textured saline breast implants and Mentor Siltex breast implants at year 2, year 3, and year 4. After 4 years, the 15.56 percent cumulative deflation rate of Poly Implant Prosthesis implants was over 3.5 times higher than the 4.31 percent deflation rate of the Mentor Siltex implants. There may be several factors contributing to the higher deflation rate seen in Poly Implant Prosthesis implants, including possible in vitro deflation before implantation and silicone shell curing technique. Nevertheless, this statistically significant deflation difference must be taken into account when balancing the risks and benefits of Poly Implant Prosthesis breast implants.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "This study provides the first large-volume (1000 implant) comparison of the deflation rates of Poly Implant Prosthesis prefilled textured saline breast implants versus a control group of Mentor Siltex textured saline implants.\nA consecutive series of 500 Poly Implant Prosthesis prefilled textured saline breast implants was compared with a consecutive series of 500 Mentor Siltex breast implants. Each breast implant was evaluated for a 4-year period, and the annual deflation rate (number of deflations during a given year divided by the total number of implants) and cumulative deflation rate (cumulative total of deflations through a given year divided by the total number of implants) were recorded. Statistical significance was calculated using the Fisher's exact test at year 1 and the chi-square analysis at years 2 through 4.\nThe cumulative deflation rates of the Poly Implant Prosthesis implants was as follows: year 1, 1.2 percent; year 2, 5.6",
        "q2": " percent; year 3, 11.4 percent; and year 4, 15.4 percent. The cumulative deflation rates of the Mentor implants was: year 1, 0.2 percent; year 2, 0.6 percent; year 3, 1.6 percent; and year 4, 4.4 percent. At year 1, the difference between deflation rates was not statistically significant (Fisher's exact test, p>0.05). However, at year 2 (chi-square, 13.29; p<0.001), year 3 (chi-square, 37.91; p<0.001), and year 4 (chi-square, 32.69; p<0.001), the difference was statistically significant.\nA comparison of 500 prefilled textured saline breast implants versus 500 standard textured saline breast implants: is there a difference in deflation rates?",
        "generated_text": "percent; year 3, 7.6 percent; and year 4, 8.8 percent. The cumulative deflation rates of the Mentor Siltex implants was as follows: year 1, 0.2 percent; year 2, 0.4 percent; year 3, 0.6 percent; and year 4, 0.8 percent. The differences in deflation rates between the 2 groups were statistically significant at all time points (p < 0.05).\nThe Poly Implant Prosthesis prefilled textured saline breast implants had a significantly higher deflation rate than the Mentor Siltex textured saline breast implants. The deflation rate of the Poly Implant Prosthesis implants was higher than the deflation rates reported in the literature for other saline breast implants.",
        "levenshtein_distance": 354
    },
    {
        "realidx": "18065862",
        "question": "We searched for factors present preoperatively which could be used to predict the intensity of postoperative pain.\nWe undertook a prospective study among all patients aged over 18 years who underwent shoulder surgery from January to September 2004 in our unit. The study included 86 patients for which the following data were noted preoperatively: past history of pain, intensity and duration of prior pain, triggering factors, localization, psychological context. The intensity of the postoperative pain was measured on a visual analog scale (VAS); measurements were noted up to 24 hours postop then at one month. Data processing was performed with Statview5.5.\nSignificant postoperative pain was correlated with a prior history of surgical pain, with duration of prior pain, with intensity of preoperative pain, and with depression.\nSignificant sustained preoperative pain can favor memory of pain leading to postoperative sensitivization of nociception neurons. Intense postoperative pain can favor the development of refractory chronic pain.\nCan the postoperative pain level be predicted preoperatively?",
        "answer": "yes",
        "answer_rationale": "Significant postoperative pain can be expected in the following situations: pain after prior surgery, presence of chronic pain sustained for more than six months, intense preoperative pain, state of depression.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "We searched for factors present preoperatively which could be used to predict the intensity of postoperative pain.\nWe undertook a prospective study among all patients aged over 18 years who underwent shoulder surgery from January to September 2004 in our unit. The study included 86 patients for which the following data were noted preoperatively: past history of pain, intensity and duration of prior pain, triggering factors, localization, psychological context. The intensity of the postoperative pain was measured on a visual analog scale (VAS);",
        "q2": " measurements were noted up to 24 hours postop then at one month. Data processing was performed with Statview5.5.\nSignificant postoperative pain was correlated with a prior history of surgical pain, with duration of prior pain, with intensity of preoperative pain, and with depression.\nSignificant sustained preoperative pain can favor memory of pain leading to postoperative sensitivization of nociception neurons. Intense postoperative pain can favor the development of refractory chronic pain.\nCan the postoperative pain level be predicted preoperatively?",
        "generated_text": "the consumption of analgesics was also noted.\nThe mean VAS score was 3.5 (range 0\u201310). The mean consumption of analgesics was 1.5 (range 0\u20136) doses of level II analgesics and 0.5 (range 0\u20133) doses of level III analgesics. The intensity of the preoperative pain was the only factor which correlated significantly with the intensity of the postoperative pain (p = 0.0001). The other factors studied",
        "levenshtein_distance": 402
    },
    {
        "realidx": "18158048",
        "question": "There is controversy surrounding the optimal management of the testicular remnant associated with the vanishing testes syndrome. Some urologists advocate the need for surgical exploration, whereas others believe this is unnecessary. These differing opinions are based on the variable reports of viable germ cell elements found within the testicular remnants. To better understand the pathology associated with this syndrome and the need for surgical management, we reviewed our experience regarding the incidence of viable germ cell elements within the testicular remnant.\nAn institutional review board-approved, retrospective review was performed of all consecutive patients undergoing exploration for a nonpalpable testis at Eastern Virginia Medical School and Geisinger Medical Center between 1994 and 2006. Patients who were found to have spermatic vessels and a vas deferens exiting a closed internal inguinal ring were included in this analysis.\nFifty-six patients underwent removal of the testicular remnant. Patient age ranged from 11 to 216 months. In 8 of the specimens (14%), we identified viable germ cell elements. In an additional 4 patients (7%), we identified seminiferous tubules without germ cell elements.\nHistologic evaluation of the testicular remnant associated with the vanishing testes syndrome: is surgical management necessary?",
        "answer": "yes",
        "answer_rationale": "In our review, we identified that a significant number of testicular remnants associated with the vanishing testes syndrome can harbor viable germ cell elements or seminiferous tubules. The exact fate of these residual elements remains unknown; however, there may exist the potential for malignant transformation. Given the potential for malignant degeneration, we believe that these remnants should be removed.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "There is controversy surrounding the optimal management of the testicular remnant associated with the vanishing testes syndrome. Some urologists advocate the need for surgical exploration, whereas others believe this is unnecessary. These differing opinions are based on the variable reports of viable germ cell elements found within the testicular remnants. To better understand the pathology associated with this syndrome and the need for surgical management, we reviewed our experience regarding the incidence of viable germ cell elements within the testicular remnant.\nAn institutional review board-approved, retrospective review was performed of all consecutive patients undergoing exploration for a nonpalpable testis at Eastern Virginia Medical School and",
        "q2": " Geisinger Medical Center between 1994 and 2006. Patients who were found to have spermatic vessels and a vas deferens exiting a closed internal inguinal ring were included in this analysis.\nFifty-six patients underwent removal of the testicular remnant. Patient age ranged from 11 to 216 months. In 8 of the specimens (14%), we identified viable germ cell elements. In an additional 4 patients (7%), we identified seminiferous tubules without germ cell elements.\nHistologic evaluation of the testicular remnant associated with the vanishing testes syndrome: is surgical management necessary?",
        "generated_text": "Children's Hospital of The King's Daughters from 2000 to 2010. Patients with vanishing testes syndrome were identified and the pathology reports were reviewed for the presence of viable germ cell elements.\nA total of 1,",
        "levenshtein_distance": 466
    },
    {
        "realidx": "18399830",
        "question": "To evaluate whether robotically assisted laparoscopic prostatectomy (RALP) is less invasive than radical retropubic prostatectomy (RRP), as experimental studies suggest that the acute phase reaction is proportional to surgery-induced tissue damage.\nBetween May and November 2006, all patients undergoing RRP or RALP in our department were prospectively assessed. Blood samples were collected 24 h before (T0), during surgery (T1), at the end of anaesthesia (T2), and 12 (T3) and 24 h after surgery (T4), and assayed for interleukin(IL)-6 and IL-1 alpha, C-reactive protein (CRP), and lactate. The Mann-Whitney U-, Student's t- and Friedman tests were used to compare continuous variables, and the Pearson chi-square and Fisher test for categorical variables, with a two-sided P<0.05 considered to indicate significance.\nIn all, 35 and 26 patients were assessed for RALP and RRP, respectively; the median (interquartile range) age was 62 (56-68) and 68.5 (59.2-71.2) years, respectively (P<0.009). Baseline levels (T0) of IL-1, IL-6, CRP and lactate were comparable in both arms. IL-6, CRP and lactates levels increased during both kinds of surgery. The mean IL-6 and CPR values were higher for RRP at T1 (P = 0.01 and 0.001), T2 (P = 0.001 and<0.001), T3 (P = 0.002 and<0.001) and T4 (P<0.001 and 0.02), respectively. Lactate was higher for RRP at T2 (P = 0.001), T3 (P = 0.001) and T4 (P = 0.004), although remaining within the normal ranges. IL-1 alpha did not change at the different sample times.\nIs robotically assisted laparoscopic radical prostatectomy less invasive than retropubic radical prostatectomy?",
        "answer": "yes",
        "answer_rationale": "This study showed for the first time that RALP induces lower tissue trauma than RRP.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To evaluate whether robotically assisted laparoscopic prostatectomy (RALP) is less invasive than radical retropubic prostatectomy (RRP), as experimental studies suggest that the acute phase reaction is proportional to surgery-induced tissue damage.\nBetween May and November 2006, all patients undergoing RRP or RALP in our department were prospectively assessed. Blood samples were collected 24 h before (T0), during surgery (T1), at the end of anaesthesia (T2), and 12 (T3) and 24 h after surgery (T4), and assayed for interleukin(IL)-6 and IL-1 alpha, C-reactive protein (CRP), and lactate. The Mann-Whitney U-, Student's t- and Friedman tests were used to compare continuous variables, and the Pearson chi-square and Fisher test for categorical variables, with a two-sided P<0.05 considered to indicate significance.\nIn all, 35 and 26 patients were assessed for RALP and RRP, respectively; the median (interquartile range) age was 62 (",
        "q2": "56-68) and 68.5 (59.2-71.2) years, respectively (P<0.009). Baseline levels (T0) of IL-1, IL-6, CRP and lactate were comparable in both arms. IL-6, CRP and lactates levels increased during both kinds of surgery. The mean IL-6 and CPR values were higher for RRP at T1 (P = 0.01 and 0.001), T2 (P = 0.001 and<0.001), T3 (P = 0.002 and<0.001) and T4 (P<0.001 and 0.02), respectively. Lactate was higher for RRP at T2 (P = 0.001), T3 (P = 0.001) and T4 (P = 0.004), although remaining within the normal ranges. IL-1 alpha did not change at the different sample times.\nIs robotically assisted laparoscopic radical prostatectomy less invasive than retropubic radical prostatectomy?",
        "generated_text": "58-66) and 63 (58-67) years, respectively (P=0.6). The median operating time was 180 (150-210) and 120 (105-150) min, respectively (P<0.001). The median estimated blood loss was 200 (150-300) and 600 (500-800) mL, respectively (P<0.001). The median hospital stay was 3 (3-4) and 5 (4-6) days, respectively (P<0.001). The median IL-6 levels at T0, T1, T2, T3 and T4 were 1.5 (1.5-1.5), 1.5 (1.5-1.5), 1.5 (1.5-1.5), 1.5 (1.5-1.5) and 1.5 (1.5-1.5) pg/mL for RALP, and 1.5 (1.5-1.5), 1.5",
        "levenshtein_distance": 495
    },
    {
        "realidx": "18568290",
        "question": "The etiology of hemodialysis (HD)-induced hypotension and hypertension remains speculative. There is mounting evidence that endothelin-1 (ET-1) may play a vital role in these hemodynamic changes. We examined the possible role of intradialytic changes of ET-1 in the pathogenesis of hypotension and rebound hypertension during HD.\nThe present study included 45 patients with end-stage renal disease (ESRD) on regular HD. They were divided according to their hemodynamic status during HD into three groups (group I had stable intradialytic hemodynamics, group II had dialysis-induced hypotension, and group III had rebound hypertension during HD). In addition, 15 healthy volunteers were included as a control group. Pulse and blood pressure were monitored before, during (every half hour), and after HD session. ET-1 level was measured at the beginning, middle, and end of HD. ET-1 was measured in the control group for comparison.\nPre-dialysis levels of ET-1 were significantly higher in dialysis patients compared to the controls (P<0.001); however, they were comparable in the three HD groups. The post-dialysis ET-1 level was not changed significantly in group I compared with predialysis values (14.49 +/- 2.04 vs. 14.33 +/- 2.23 pg/ml; P = NS), while the ET-1 concentration decreased significantly in group II and increased in group III in comparison to predialysis values (8.56 +/- 1.44 vs. 11.75 +/- 2.51; 16.39 +/- 3.12 vs. 11.93 +/- 2.11 pg/ml, respectively; P<0.001).\nIs there a role for endothelin-1 in the hemodynamic changes during hemodialysis?",
        "answer": "maybe",
        "answer_rationale": "Altered ET-1 levels may be involved in the pathogenesis of rebound hypertension and hypotension during HD.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "The etiology of hemodialysis (HD)-induced hypotension and hypertension remains speculative. There is mounting evidence that endothelin-1 (ET-1) may play a vital role in these hemodynamic changes. We examined the possible role of intradialytic changes of ET-1 in the pathogenesis of hypotension and rebound hypertension during HD.\nThe present study included 45 patients with end-stage renal disease (ESRD) on regular HD. They were divided according to their hemodynamic status during HD into three groups (group I had stable intradialytic hemodynamics, group II had dialysis-induced hypotension, and group III had rebound hypertension during HD). In addition, 15 healthy volunteers were included as a control group. Pulse and blood pressure were monitored before, during (every half hour), and after HD session. ET-1 level was measured at the beginning, middle, and end of HD. ET-",
        "q2": "1 was measured in the control group for comparison.\nPre-dialysis levels of ET-1 were significantly higher in dialysis patients compared to the controls (P<0.001); however, they were comparable in the three HD groups. The post-dialysis ET-1 level was not changed significantly in group I compared with predialysis values (14.49 +/- 2.04 vs. 14.33 +/- 2.23 pg/ml; P = NS), while the ET-1 concentration decreased significantly in group II and increased in group III in comparison to predialysis values (8.56 +/- 1.44 vs. 11.75 +/- 2.51; 16.39 +/- 3.12 vs. 11.93 +/- 2.11 pg/ml, respectively; P<0.001).\nIs there a role for endothelin-1 in the hemodynamic changes during hemodialysis?",
        "generated_text": "1 level was also measured in the dialysate at the end of HD.\nThe mean predialysis ET-1 level was significantly higher in the three patient groups than in the control group (P < 0.001). The mean predialysis ET-1 level was significantly higher in group III than in groups I and II (P < 0.001). The mean intradialytic ET-1 level was significantly higher in group III than in groups I and II (P < 0.001). The mean postdialysis ET-1 level was significantly higher in group III than in groups I and II (P < 0.001). The mean ET-1 level in the dialysate was significantly higher in group III than in groups I and II (P < 0.001). There was a significant positive correlation between the mean intradialytic ET-1 level and the mean intradialytic systolic blood pressure (SBP",
        "levenshtein_distance": 528
    },
    {
        "realidx": "18667100",
        "question": "Suicide is a leading cause of death and has been strongly associated with affective disorders. The influence of affective disorder polarity on subsequent suicide attempts or completions and any differential effect of suicide risk factors by polarity were assessed in a prospective cohort.\nParticipants with major affective disorders in the National Institute of Mental Health (NIMH) Collaborative Depression Study (CDS) were followed prospectively for up to 25 years. A total of 909 participants meeting prospective diagnostic criteria for major depressive and bipolar disorders were followed through 4204 mood cycles. Suicidal behavior was defined as suicide attempts or completions. Mixed-effects, grouped-time survival analysis assessed risk of suicidal behavior and differential effects of risk factors for suicidal behavior by polarity. In addition to polarity, the main effects of age, gender, hopelessness, married status, prior suicide attempts and active substance abuse were modeled, with mood cycle as the unit of analysis.\nAfter controlling for age of onset, there were no differences in prior suicide attempts by polarity although bipolar participants had more prior severe attempts. During follow-up, 40 cycles ended in suicide and 384 cycles contained at least one suicide attempt. Age, hopelessness and active substance abuse but not polarity predicted suicidal behavior. The effects of risk factors did not differ by polarity.\nDo risk factors for suicidal behavior differ by affective disorder polarity?",
        "answer": "no",
        "answer_rationale": "Bipolarity does not independently influence risk of suicidal behavior or alter the influence of well-established suicide risk factors within affective disorders. Suicide risk assessment strategies may continue to appraise these common risk factors without regard to mood polarity.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Suicide is a leading cause of death and has been strongly associated with affective disorders. The influence of affective disorder polarity on subsequent suicide attempts or completions and any differential effect of suicide risk factors by polarity were assessed in a prospective cohort.\nParticipants with major affective disorders in the National Institute of Mental Health (NIMH) Collaborative Depression Study (CDS) were followed prospectively for up to 25 years. A total of 909 participants meeting prospective diagnostic criteria for major depressive and bipolar disorders were followed through 4204 mood cycles. Suicidal behavior was defined as suicide attempts or completions. Mixed-effects, grouped-time survival analysis assessed risk of suicidal behavior",
        "q2": " and differential effects of risk factors for suicidal behavior by polarity. In addition to polarity, the main effects of age, gender, hopelessness, married status, prior suicide attempts and active substance abuse were modeled, with mood cycle as the unit of analysis.\nAfter controlling for age of onset, there were no differences in prior suicide attempts by polarity although bipolar participants had more prior severe attempts. During follow-up, 40 cycles ended in suicide and 384 cycles contained at least one suicide attempt. Age, hopelessness and active substance abuse but not polarity predicted suicidal behavior. The effects of risk factors did not differ by polarity.\nDo risk factors for suicidal behavior differ by affective disorder polarity?",
        "generated_text": "by polarity and cycle. Risk factors for suicidal behavior were assessed using generalized estimating equations (GEE) logistic regression.\nBipolar depression and mixed state carried the highest risk of suicidal behavior, followed by major depressive disorder and bipolar mania. Risk factors for suicidal behavior included younger age, female gender, suicidal ideation, prior suicide attempts, and comorbid substance use disorder. The effect of polarity on suicidal behavior was not modified by any risk factors.\nBipolar depression and mixed state carried the highest risk of suicidal behavior. Risk factors for suicidal behavior were consistent across polarities. These findings highlight the importance of considering the full spectrum of bipolarity in suicide risk assessment and prevention.",
        "levenshtein_distance": 580
    },
    {
        "realidx": "18693227",
        "question": "This study was performed to describe the treatment plan modifications after a geriatric oncology clinic. Assessment of health and functional status and cancer assessment was performed in older cancer patients referred to a cancer center.\nBetween June 2004 and May 2005, 105 patients 70 years old or older referred to a geriatric oncology consultation at the Institut Curie cancer center were included. Functional status, nutritional status, mood, mobility, comorbidity, medication, social support, and place of residence were assessed. Oncology data and treatment decisions were recorded before and after this consultation. Data were analyzed for a possible correlation between one domain of the assessment and modification of the treatment plan.\nPatient characteristics included a median age of 79 years and a predominance of women with breast cancer. About one half of patients had an independent functional status. Nearly 15% presented severe undernourishment. Depression was suspected in 53.1% of cases. One third of these patients had>2 chronic diseases, and 74% of patients took>or =3 medications. Of the 93 patients with an initial treatment decision, the treatment plan was modified for 38.7% of cases after this assessment. Only body mass index and the absence of depressive symptoms were associated with a modification of the treatment plan.\nDoes a geriatric oncology consultation modify the cancer treatment plan for elderly patients?",
        "answer": "yes",
        "answer_rationale": "The geriatric oncology consultation led to a modification of the cancer treatment plan in more than one third of cases. Further studies are needed to determine whether these modifications improve the outcome of these older patients.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "This study was performed to describe the treatment plan modifications after a geriatric oncology clinic. Assessment of health and functional status and cancer assessment was performed in older cancer patients referred to a cancer center.\nBetween June 2004 and May 2005, 105 patients 70 years old or older referred to a geriatric oncology consultation at the Institut Curie cancer center were included. Functional status, nutritional status, mood, mobility, comorbidity, medication, social support, and place of residence were assessed. Oncology data and treatment decisions were recorded before and after this consultation. Data were analyzed for a possible correlation between one domain of the assessment and modification of the treatment plan.\nPatient characteristics included",
        "q2": " a median age of 79 years and a predominance of women with breast cancer. About one half of patients had an independent functional status. Nearly 15% presented severe undernourishment. Depression was suspected in 53.1% of cases. One third of these patients had>2 chronic diseases, and 74% of patients took>or =3 medications. Of the 93 patients with an initial treatment decision, the treatment plan was modified for 38.7% of cases after this assessment. Only body mass index and the absence of depressive symptoms were associated with a modification of the treatment plan.\nDoes a geriatric oncology consultation modify the cancer treatment plan for elderly patients?",
        "generated_text": "a median age of 80 years (range, 70\u201395 years), 60% women, 60% with metastatic disease, and 40% with breast cancer. The treatment plan was modified in 49% of the patients. The most frequent modifications were a change in the treatment modality (chemotherapy to hormone therapy, 12%) and a dose reduction (10%). The most frequent reasons for modification were comorbidity (24%) and loss of autonomy (22%). The domains of the geriatric assessment that were significantly associated with a modification of the treatment plan were functional status (P = 0.0001), comorbidities (P = 0.0001), and nutritional status",
        "levenshtein_distance": 454
    },
    {
        "realidx": "20297950",
        "question": "To investigate the effect of fenofibrate on sleep apnoea indices.\nProof-of-concept study comprising a placebo run-in period (1 week, 5 weeks if fibrate washout was required) and a 4-week randomized, double-blind treatment period. Thirty-four subjects (mean age 55 years, body mass index 34 kg/m 2 , fasting triglycerides 3.5 mmol/L) with diagnosed sleep apnoea syndrome not treated with continuous positive airways pressure were enrolled and randomized to once daily treatment with fenofibrate (145 mg NanoCrystal(R) tablet) or placebo. Overnight polysomnography, computerized attention/vigilance tests and blood sampling for measurement of lipids, insulin, fasting plasma glucose and fibrinogen were performed at the end of each study period.\nNCT00816829.\nAs this was an exploratory study, a range of sleep variables were evaluated. The apnoea/hypopnoea index (AHI) and percentage of time spent with arterial oxygen saturation (SpO(2))<90% were relevant as they have been evaluated in other clinical trials. Other variables included total apnoeas, hypopnoeas and oxygen desaturations, and non-cortical micro-awakenings related to respiratory events per hour.\nFenofibrate treatment significantly reduced the percentage of time with SpO(2)<90% (from 9.0% to 3.5% vs. 10.0% to 11.5% with placebo, p = 0.007), although there was no significant change in the AHI (reduction vs. control 14% (95%CI -47 to 40%, p = 0.533). Treatment reduced obstructive apnoeas (by 44%, from 18.5 at baseline to 15.0 at end of treatment vs. 29.0 to 30.5 on placebo, p = 0.048), and non-cortical micro-awakenings per hour (from 23.5 to 18.0 vs. 24.0 to 25.0 with placebo, p = 0.004). Other sleep variables were not significantly influenced by fenofibrate.\nExploratory study in patients with mild to moderate sleep apnoea, limited treatment duration; concomitant hypnotic treatment (35%); lack of correction for multiplicity of testing.\nProof of concept study: does fenofibrate have a role in sleep apnoea syndrome?",
        "answer": "yes",
        "answer_rationale": "The consistent direction of change in sleep indices in this proof-of-concept study may support further investigation of fenofibrate in moderate to severe sleep apnoea syndrome.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To investigate the effect of fenofibrate on sleep apnoea indices.\nProof-of-concept study comprising a placebo run-in period (1 week, 5 weeks if fibrate washout was required) and a 4-week randomized, double-blind treatment period. Thirty-four subjects (mean age 55 years, body mass index 34 kg/m 2 , fasting triglycerides 3.5 mmol/L) with diagnosed sleep apnoea syndrome not treated with continuous positive airways pressure were enrolled and randomized to once daily treatment with fenofibrate (145 mg NanoCrystal(R) tablet) or placebo. Overnight polysomnography, computerized attention/vigilance tests and blood sampling for measurement of lipids, insulin, fasting plasma glucose and fibrinogen were performed at the end of each study period.\nNCT00816829.\nAs this was an exploratory study, a range of sleep variables were evaluated. The apnoea/hypopnoea index (AHI) and percentage of time spent with arterial oxygen saturation (SpO(2))<90% were relevant as they have been evaluated in other clinical trials. Other variables included total apnoeas, hypopnoeas and oxygen desaturations, and non-cortical micro-awakenings related",
        "q2": " to respiratory events per hour.\nFenofibrate treatment significantly reduced the percentage of time with SpO(2)<90% (from 9.0% to 3.5% vs. 10.0% to 11.5% with placebo, p = 0.007), although there was no significant change in the AHI (reduction vs. control 14% (95%CI -47 to 40%, p = 0.533). Treatment reduced obstructive apnoeas (by 44%, from 18.5 at baseline to 15.0 at end of treatment vs. 29.0 to 30.5 on placebo, p = 0.048), and non-cortical micro-awakenings per hour (from 23.5 to 18.0 vs. 24.0 to 25.0 with placebo, p = 0.004). Other sleep variables were not significantly influenced by fenofibrate.\nExploratory study in patients with mild to moderate sleep apnoea, limited treatment duration; concomitant hypnotic treatment (35%); lack of correction for multiplicity of testing.\nProof of concept study: does fenofibrate have a role in sleep apnoea syndrome?",
        "generated_text": "to respiratory events. The primary efficacy variable was the change in AHI from baseline to the end of the treatment period. Secondary efficacy variables included changes in other sleep variables, attention/vigilance test results and metabolic parameters.\nFenofibrate treatment did not significantly change the AHI (mean change from baseline: fenofibrate -0.3 events/h, placebo -1.3 events/h; p = 0.78) or the percentage of time spent with SpO(2)<90% (fenofibrate -0.3%, placebo -0.1%; p = 0.90). There were no significant differences between fenofibrate and placebo for any of the other sleep variables or attention/vigilance test results. Fenofibrate significantly reduced plasma triglycerides (-1.3 mmol/L, p<0.0001), total cholesterol (-0.3 mmol/L, p = 0.0002), fibrinogen (-0.6 g/L, p = 0.0002) and fasting plasma glucose (-0.3 mmol/L, p = 0.02) compared with placebo.\nFenofibrate treatment did not improve sleep apnoea indices in patients with sleep apnoea syndrome.",
        "levenshtein_distance": 714
    },
    {
        "realidx": "20337202",
        "question": "To determine the duration of continuing pregnancy after antenatal corticosteroid (AC) administration and to evaluate the potential opportunity for rescue AC.\nRetrospective analysis of women at 24-32 weeks' gestation who received AC at one institution.\nSix hundred ninety-two women received AC. Two hundred forty-seven (35.7%) delivered at>or = 34 weeks' gestation. Three hundred twenty-one (46.4%) delivered within 1 week of AC; 92 of those women (13.3%) delivered within 24 hours. Only 124 (17.9%) remained pregnant 1 week after AC and delivered at<34 weeks. The latter were compared to women delivering>2 week after AC but>or = 34 weeks. More likely to deliver at<34 weeks were those women who received AC for premature preterm rupture of membranes (OR 3.83, 95% CI 2.06-7.17), twins (OR 2.90, 95% CI 1.42-5.95) or before 28 weeks (OR 2.21, 95% CI 1.38-3.52).\nContinuation of pregnancy after antenatal corticosteroid administration: opportunity for rescue?",
        "answer": "maybe",
        "answer_rationale": "Rescue AC may apply to only 18% of cases, and we identified subsets of more likely candidates.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "To determine the duration of continuing pregnancy after antenatal corticosteroid (AC) administration and to evaluate the potential opportunity for rescue AC.\nRetrospective analysis of women at 24-32 weeks' gestation who received AC at one institution.\nSix hundred ninety-two women received AC. Two hundred forty-seven (35.7%) delivered at>or = 34 weeks' gestation. Three hundred twenty-one (46.4%) delivered within 1 week of AC; 92 of those women (13.3%) delivered within 24 hours. Only 124 (17.9%) remained pregnant 1 week after AC and",
        "q2": " delivered at<34 weeks. The latter were compared to women delivering>2 week after AC but>or = 34 weeks. More likely to deliver at<34 weeks were those women who received AC for premature preterm rupture of membranes (OR 3.83, 95% CI 2.06-7.17), twins (OR 2.90, 95% CI 1.42-5.95) or before 28 weeks (OR 2.21, 95% CI 1.38-3.52).\nContinuation of pregnancy after antenatal corticosteroid administration: opportunity for rescue?",
        "generated_text": "delivered between 1 and 2 weeks after AC. One hundred twenty (17.3%) remained pregnant>or = 2 weeks after AC. Of the 120 women who remained pregnant>or = 2 weeks after AC, 30 (25%) delivered between 2 and 3 weeks after AC, 20 (16.7%) delivered between 3 and 4 weeks after AC, and 70 (58.3%) delivered>or = 4 weeks after AC.\nThe majority of women who receive AC deliver within 1 week. Only 17.3% remain pregnant>or = 2 weeks after",
        "levenshtein_distance": 333
    },
    {
        "realidx": "21420186",
        "question": "Sporadic data present in literature report how preterm birth and low birth weight are risk factors for the development of cardiovascular diseases in later life. High levels of asymmetric dimethylarginine (ADMA), a strong inhibitor of nitric oxide synthesis, are associated with the future development of adverse cardiovascular events and cardiac death.\n1) to verify the presence of a statistically significant difference between ADMA levels in young adults born preterm at extremely low birth weight (<1000 g; ex-ELBW) and those of a control group of healthy adults born at term (C) and 2) to seek correlations between ADMA levels in ex-ELBW and anthropometric and clinical parameters (gender, chronological age, gestational age, birth weight, and duration of stay in Neonatal Intensive Care Unit).\nThirty-two ex-ELBW subjects (11 males [M] and 21 females [F], aged 17-29years, mean age 22.2 \u00b1 2.3 years) were compared with 25 C (7 M and 18F). ADMA levels were assessed by high-performance liquid chromatography with highly sensitive laser fluorescent detection.\nADMA levels were reduced in ex-ELBW subjects compared to C (0.606+0.095 vs 0.562+0.101 \u03bcmol/L, p<0.05), and significantly correlated inversely with gestational age (r=-0.61, p<0.00001) and birth weight (r=-0.57, p<0.0002).\nCould ADMA levels in young adults born preterm predict an early endothelial dysfunction?",
        "answer": "yes",
        "answer_rationale": "Our findings reveal a significant decrease in ADMA levels of ex-ELBW subjects compared to C, underlining a probable correlation with preterm birth and low birth weight. Taken together, these results may underlie the onset of early circulatory dysfunction predictive of increased cardiovascular risk.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Sporadic data present in literature report how preterm birth and low birth weight are risk factors for the development of cardiovascular diseases in later life. High levels of asymmetric dimethylarginine (ADMA), a strong inhibitor of nitric oxide synthesis, are associated with the future development of adverse cardiovascular events and cardiac death.\n1) to verify the presence of a statistically significant difference between ADMA levels in young adults born preterm at extremely low birth weight (<1000 g; ex-ELBW) and those of a control group of healthy adults born at term (C) and 2) to seek correlations between ADMA levels in ex-ELBW and anthropometric and clinical parameters (gender, chronological age, gestational age, birth weight, and duration of stay in Neonatal Intensive Care Unit).\nThirty-two ex-",
        "q2": "ELBW subjects (11 males [M] and 21 females [F], aged 17-29years, mean age 22.2 \u00b1 2.3 years) were compared with 25 C (7 M and 18F). ADMA levels were assessed by high-performance liquid chromatography with highly sensitive laser fluorescent detection.\nADMA levels were reduced in ex-ELBW subjects compared to C (0.606+0.095 vs 0.562+0.101 \u03bcmol/L, p<0.05), and significantly correlated inversely with gestational age (r=-0.61, p<0.00001) and birth weight (r=-0.57, p<0.0002).\nCould ADMA levels in young adults born preterm predict an early endothelial dysfunction?",
        "generated_text": "ELBW (mean age 24.3 \u00b1 1.6 years) and 32 C (mean age 24.5 \u00b1 1.6 years) were enrolled. ADMA levels were determined by high-performance liquid chromatography.\nADMA levels were significantly higher in ex-ELBW than in C (0.66 \u00b1 0.12 vs. 0.55 \u00b1 0.08 \u03bcmol/L; p < 0.001). In ex-ELBW, ADMA levels were significantly higher in males than in females (0.72 \u00b1 0.12 vs. 0.60 \u00b1 0.08 \u03bcmol/L; p = 0.001). In ex-ELBW, ADMA levels were significantly correlated with gestational age (r = \u22120.40;",
        "levenshtein_distance": 349
    },
    {
        "realidx": "21459725",
        "question": "Xanthogranulomatous cholecystitis (XGC) is an uncommon variant of chronic cholecystitis, characterized by marked thickening of the gallbladder wall and dense local adhesions. It often mimics a gallbladder carcinoma (GBC), and may coexist with GBC, leading to a diagnostic dilemma. Furthermore, the premalignant nature of this entity is not known. This study was undertaken to assess the p53, PCNA and beta-catenin expression in XGC in comparison to GBC and chronic inflammation.\nSections from paraffin-embedded blocks of surgically resected specimens of GBC (69 cases), XGC (65), chronic cholecystitis (18) and control gallbladder (10) were stained with the monoclonal antibodies to p53 and PCNA, and a polyclonal antibody to beta-catenin. p53 expression was scored as the percentage of nuclei stained. PCNA expression was scored as the product of the percentage of nuclei stained and the intensity of the staining (1-3). A cut-off value of 80 for this score was taken as a positive result. Beta-catenin expression was scored as type of expression-membranous, cytoplasmic or nuclear staining.\np53 mutation was positive in 52% of GBC cases and 3% of XGC, but was not expressed in chronic cholecystitis and control gallbladders. p53 expression was lower in XGC than in GBC (P<0.0001). PCNA expression was seen in 65% of GBC cases and 11% of XGC, but not in chronic cholecystitis and control gallbladders. PCNA expression was higher in GBC than XGC (P=0.0001), but there was no significant difference between the XGC, chronic cholecystitis and control gallbladder groups. Beta-catenin expression was positive in the GBC, XGC, chronic cholecystitis and control gallbladder groups. But the expression pattern in XGC, chronic cholecystitis and control gallbladders was homogenously membranous, whereas in GBC the membranous expression pattern was altered to cytoplasmic and nuclear.\nXanthogranulomatous cholecystitis: a premalignant condition?",
        "answer": "no",
        "answer_rationale": "The expression of p53, PCNA and beta-catenin in XGC was significantly different from GBC and similar to chronic cholecystitis, thus indicating the inflammatory nature of XGC and may not support a premalignant nature of the lesion.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Xanthogranulomatous cholecystitis (XGC) is an uncommon variant of chronic cholecystitis, characterized by marked thickening of the gallbladder wall and dense local adhesions. It often mimics a gallbladder carcinoma (GBC), and may coexist with GBC, leading to a diagnostic dilemma. Furthermore, the premalignant nature of this entity is not known. This study was undertaken to assess the p53, PCNA and beta-catenin expression in XGC in comparison to GBC and chronic inflammation.\nSections from paraffin-embedded blocks of surgically resected specimens of GBC (69 cases), XGC (65), chronic cholecystitis (18) and control gallbladder (10) were stained with the monoclonal antibodies to p53 and PCNA, and a polyclonal antibody to beta-catenin. p53 expression was scored as the percentage of nuclei stained. PCNA expression was scored as the product of the percentage of nuclei stained and the intensity of the staining (1-3). A cut-off value of 80 for this score was taken as a positive result. Beta-catenin expression was scored as",
        "q2": " type of expression-membranous, cytoplasmic or nuclear staining.\np53 mutation was positive in 52% of GBC cases and 3% of XGC, but was not expressed in chronic cholecystitis and control gallbladders. p53 expression was lower in XGC than in GBC (P<0.0001). PCNA expression was seen in 65% of GBC cases and 11% of XGC, but not in chronic cholecystitis and control gallbladders. PCNA expression was higher in GBC than XGC (P=0.0001), but there was no significant difference between the XGC, chronic cholecystitis and control gallbladder groups. Beta-catenin expression was positive in the GBC, XGC, chronic cholecystitis and control gallbladder groups. But the expression pattern in XGC, chronic cholecystitis and control gallbladders was homogenously membranous, whereas in GBC the membranous expression pattern was altered to cytoplasmic and nuclear.\nXanthogranulomatous cholecystitis: a premalignant condition?",
        "generated_text": "the percentage of cells showing membranous staining.\np53 expression was seen in 0%, 0%, 0% and 30.4% of control gallbladder, chronic cholecystitis, XGC and GBC, respectively. PCNA expression was seen in 0%, 0%, 0% and 30.4% of control gallbladder, chronic cholecystitis, XGC and GBC, respectively. Beta-catenin expression was seen in 100%, 100%, 100% and 0% of control gallbladder, chronic cholecystitis, XGC and GBC, respectively.\nXGC does not show p53 or PCNA expression, and shows normal beta-catenin expression, similar to chronic cholecystitis. This suggests that XGC is not a premalignant condition.",
        "levenshtein_distance": 591
    },
    {
        "realidx": "21658267",
        "question": "There are three main service delivery channels: clinical services, outreach, and family and community. To determine which delivery channels are associated with the greatest reductions in under-5 mortality rates (U5MR), we used data from sequential population-based surveys to examine the correlation between changes in coverage of clinical, outreach, and family and community services and in U5MR for 27 high-burden countries.\nHousehold survey data were abstracted from serial surveys in 27 countries. Average annual changes (AAC) between the most recent and penultimate survey were calculated for under-five mortality rates and for 22 variables in the domains of clinical, outreach, and family- and community-based services. For all 27 countries and a subset of 19 African countries, we conducted principal component analysis to reduce the variables into a few components in each domain and applied linear regression to assess the correlation between changes in the principal components and changes in under-five mortality rates after controlling for multiple potential confounding factors.\nAAC in under 5-mortality varied from 6.6% in Nepal to -0.9% in Kenya, with six of the 19 African countries all experiencing less than a 1% decline in mortality. The strongest correlation with reductions in U5MR was observed for access to clinical services (all countries: p = 0.02, r\u00b2 = 0.58; 19 African countries p<0.001, r\u00b2 = 0.67). For outreach activities, AAC U5MR was significantly correlated with antenatal care and family planning services, while AAC in immunization services showed no association. In the family- and community services domain, improvements in breastfeeding were associated with significant changes in mortality in the 30 countries but not in the African subset; while in the African countries, nutritional status improvements were associated with a significant decline in mortality.\nDo improvements in outreach, clinical, and family and community-based services predict improvements in child survival?",
        "answer": "yes",
        "answer_rationale": "Our findings support the importance of increasing access to clinical services, certain outreach services and breastfeeding and, in Africa, of improving nutritional status. Integrated programs that emphasize these services may lead to substantial mortality declines.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "There are three main service delivery channels: clinical services, outreach, and family and community. To determine which delivery channels are associated with the greatest reductions in under-5 mortality rates (U5MR), we used data from sequential population-based surveys to examine the correlation between changes in coverage of clinical, outreach, and family and community services and in U5MR for 27 high-burden countries.\nHousehold survey data were abstracted from serial surveys in 27 countries. Average annual changes (AAC) between the most recent and penultimate survey were calculated for under-five mortality rates and for 22 variables in the domains of clinical, outreach, and family- and community-based services. For all 27 countries and a subset of 19 African countries, we conducted principal component analysis to reduce the variables into a few components in each domain and applied linear regression to assess the correlation between changes in the principal components and changes in under-five mortality rates after controlling for multiple potential confounding factors.\n",
        "q2": "AAC in under 5-mortality varied from 6.6% in Nepal to -0.9% in Kenya, with six of the 19 African countries all experiencing less than a 1% decline in mortality. The strongest correlation with reductions in U5MR was observed for access to clinical services (all countries: p = 0.02, r\u00b2 = 0.58; 19 African countries p<0.001, r\u00b2 = 0.67). For outreach activities, AAC U5MR was significantly correlated with antenatal care and family planning services, while AAC in immunization services showed no association. In the family- and community services domain, improvements in breastfeeding were associated with significant changes in mortality in the 30 countries but not in the African subset; while in the African countries, nutritional status improvements were associated with a significant decline in mortality.\nDo improvements in outreach, clinical, and family and community-based services predict improvements in child survival?",
        "generated_text": "The AAC in U5MR for all 27 countries was \u22123.6% (range: \u22127.0% to 0.1%). The AAC in coverage of clinical services was 1.1% (range: \u22120.6% to 3.0%), for outreach services 1.7% (range: \u22120.6% to 4.5%), and for family and community services 0.3% (range: \u22120.3% to 1.0%). In the regression analysis, the principal components for clinical services (P = 0.01) and family and community services (P = 0.02) were significantly associated with reductions in U5MR for all 27 countries. For the 19 African countries, the principal components for clinical services (P = 0.01) and outreach services (P = 0.01) were significantly associated with reductions in U5MR.\nThe results",
        "levenshtein_distance": 634
    },
    {
        "realidx": "21726930",
        "question": "Endometrial polyp is a common cause of abnormal uterine bleeding, but the etiology and pathogenesis remain unclear. Vascular endothelial growth factor (VEGF) is angiogenic, related to thick walled vessels and transforming growth factor-beta1 (TGF-\u03b21) is related to fibrotic tissue, which are characteristics of endometrial polyps. The primary objective of this study was to find out if endometrial polyp formation is associated with increased expression of VEGF or TGF-\u03b21, or both. A secondary objective is to determine if the changes are related to steroid receptor expression.\nThis prospective study compared VEGF and TGF-\u03b21 expression of endometrial polyps and adjacent endometrial tissue in 70 premenopausal women. The comparison of results was separately made for endometrium specimens obtained in the proliferative and secretory phases. The results were correlated with the steroid receptors (estrogen receptor and progesterone receptor) expression.\nThe score of VEGF in glandular cells of endometrial polyps was significantly higher than the score in adjacent endometrium, both in the proliferative phase (P<0.001) and the secretory phase (P=0.03); the score of VEGF in stromal cells of endometrial polyps was significantly higher than the score in adjacent endometrium only in proliferative phase (P=0.006). The score of TGF-\u03b21 in glandular cells of endometrial polyps was significantly higher than the score in adjacent endometrium in proliferative phase (P=0.02); whereas the score of TGF-\u03b21 in stromal cells of endometrial polyps was significantly higher than the score in adjacent endometrium, both in the proliferative phase (P=0.006) and the secretory phase (P=0.008). There was a significant correlation between the expression of steroid receptors and VEGF and TGF-\u03b21 (Spearman's correlation P<0.001 and P<0.05, respectively).\nIs endometrial polyp formation associated with increased expression of vascular endothelial growth factor and transforming growth factor-beta1?",
        "answer": "yes",
        "answer_rationale": "There was increased expression of TGF-\u03b21 and VEGF in polyps compared to adjacent normal endometrial tissue. It suggested that these cytokines might play a role in endometrial polyp formation. In addition, there was a significant correlation between steroid receptor expression and VEGF and TGF-\u03b21 expression.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Endometrial polyp is a common cause of abnormal uterine bleeding, but the etiology and pathogenesis remain unclear. Vascular endothelial growth factor (VEGF) is angiogenic, related to thick walled vessels and transforming growth factor-beta1 (TGF-\u03b21) is related to fibrotic tissue, which are characteristics of endometrial polyps. The primary objective of this study was to find out if endometrial polyp formation is associated with increased expression of VEGF or TGF-\u03b21, or both. A secondary objective is to determine if the changes are related to steroid receptor expression.\nThis prospective study compared VEGF and TGF-\u03b21 expression of endometrial polyps and adjacent endometrial tissue in 70 premenopausal women. The comparison of results was separately made for endometrium specimens obtained in the proliferative and secretory phases. The results were correlated with the steroid receptors (estrogen receptor and progesterone receptor) expression.\nThe score of VEGF in glandular cells of endometrial polyps was significantly higher than the score in adjacent",
        "q2": " endometrium, both in the proliferative phase (P<0.001) and the secretory phase (P=0.03); the score of VEGF in stromal cells of endometrial polyps was significantly higher than the score in adjacent endometrium only in proliferative phase (P=0.006). The score of TGF-\u03b21 in glandular cells of endometrial polyps was significantly higher than the score in adjacent endometrium in proliferative phase (P=0.02); whereas the score of TGF-\u03b21 in stromal cells of endometrial polyps was significantly higher than the score in adjacent endometrium, both in the proliferative phase (P=0.006) and the secretory phase (P=0.008). There was a significant correlation between the expression of steroid receptors and VEGF and TGF-\u03b21 (Spearman's correlation P<0.001 and P<0.05, respectively).\nIs endometrial polyp formation associated with increased expression of vascular endothelial growth factor and transforming growth factor-beta1?",
        "generated_text": "endometrium in the proliferative phase (P = 0.001). The score of TGF-\u03b21 in stromal cells of endometrial polyps was significantly higher than the score in adjacent endometrium in the proliferative phase (P = 0.001). The expression of VEGF and TGF-\u03b21 in endometrial polyps was not related to the expression of steroid receptors.\nThe increased expression of VEGF and TGF-\u03b21 in endometrial polyps suggests that these growth factors may play a role in the pathogenesis of endometrial polyps. The expression of VEGF and TGF-\u03b21 in endometrial polyps was not related to the expression of steroid receptors.",
        "levenshtein_distance": 500
    },
    {
        "realidx": "21850494",
        "question": "Hepatorenal syndrome (HRS) is the functional renal failure associated with advanced cirrhosis and has also been described in fulminant hepatic failure. Without liver transplantation its prognosis is dismal. Our study included patients with type 1 HRS associated with cirrhosis, who were not liver transplant candidates.AIM: To identify variables associated with improved survival.\nSixty-eight patients fulfilled the revised Ascites Club Criteria for type 1 HRS. None of them was suitable for liver transplantation. All the patients were treated with combinations of: albumin, midodrine and octreotide, pressors, and hemodialysis.\nMedian survival was 13 days for the whole group. Survival varied with the end-stage liver disease (ESLD) etiology: autoimmune, 49 days, cardiac cirrhosis, 22 days, idiopathic, 15.5 days, viral, 15 days, hepatitis C and alcohol, 14.5 days, alcohol 8 days, and neoplasia 4 days (p = 0.048). Survival of HRS associated with alcoholic liver disease versus other etiologies was not statistically significant (p = 0.1). Increased serum creatinine (p = 0.02) and urinary sodium 6-10 mEq/l (p = 0.027) at the initiation of therapy were prognostic factors for mortality. HRS treatment modalities (p = 0.73), use of dialysis (p = 0.56), dialysis modality (p = 0.35), use of vasopressors (p = 0.26), pre-existing renal disease (p = 0.49), gender (p = 0.90), and age (p = 0.57) were not associated with survival.\nHepatorenal syndrome: are we missing some prognostic factors?",
        "answer": "yes",
        "answer_rationale": "We report for the first time ESLD etiology as a prognostic factor for survival. The renal function (expressed as serum creatinine) and urinary Na (<5 mEq/l) at the time of diagnosis were found to be associated with survival, suggesting that early treatment might increase survival.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Hepatorenal syndrome (HRS) is the functional renal failure associated with advanced cirrhosis and has also been described in fulminant hepatic failure. Without liver transplantation its prognosis is dismal. Our study included patients with type 1 HRS associated with cirrhosis, who were not liver transplant candidates.AIM: To identify variables associated with improved survival.\nSixty-eight patients fulfilled the revised Ascites Club Criteria for type 1 HRS. None of them was suitable for liver transplantation. All the patients were treated with combinations of: albumin, midodrine and octreotide, pressors, and hemodialysis.\nMedian survival was 13 days for the whole group. Survival varied with the end-stage liver disease (ESLD) etiology: autoimmune, 49 days, cardiac cirrhosis, 22 days, idiopathic, 15.5 days, viral, 15 days, hepatitis C and",
        "q2": " alcohol, 14.5 days, alcohol 8 days, and neoplasia 4 days (p = 0.048). Survival of HRS associated with alcoholic liver disease versus other etiologies was not statistically significant (p = 0.1). Increased serum creatinine (p = 0.02) and urinary sodium 6-10 mEq/l (p = 0.027) at the initiation of therapy were prognostic factors for mortality. HRS treatment modalities (p = 0.73), use of dialysis (p = 0.56), dialysis modality (p = 0.35), use of vasopressors (p = 0.26), pre-existing renal disease (p = 0.49), gender (p = 0.90), and age (p = 0.57) were not associated with survival.\nHepatorenal syndrome: are we missing some prognostic factors?",
        "generated_text": "alcohol, 12 days, alcohol, 10 days, and hepatitis B, 5 days. Patients with autoimmune hepatitis had significantly better survival than the rest of the group (P=0.0001). Patients with a MELD score of 20 or less had a median survival of 30 days, whereas those with a MELD score of 21 or more had a median survival of 10 days (P=0.0001). Patients with a serum creatinine of 3 mg/dL or less had a median survival of 20 days, whereas those with a serum creatinine of more than 3 mg/dL had a median survival of 10 days (P=0.0001). Patients with a serum sodium of 130 mEq/L or more had a median survival of 20 days, whereas those with a serum sodium of less than 130 mEq/L had a median survival of 10 days (P=0.0001). Patients",
        "levenshtein_distance": 538
    },
    {
        "realidx": "22154448",
        "question": "To study the risks of haemodynamic instability, and the possible occurrence of spinal haematoma, meningitis and epidural abscess when epidural analgesia is performed for cytoreductive surgery and hyperthermic intraperitoneal chemotherapy (HIPEC).\nWe retrospectively analyzed the data of 35 patients treated by HIPEC with oxaliplatin or cisplatin. An epidural catheter was inserted before induction of general anaesthesia. Postoperatively, a continuous epidural infusion of ropivacain, then a patient-controlled epidural analgesia were started.\nThe epidural catheter was used peroperatively before HIPEC in 12 subjects (34%), and after HIPEC in 23 subjects (66%). The median dose of ropivacain given peroperatively in the epidural catheter was 40 mg (30-75). Norepinephrin was used in two subjects (6%) peroperatively (median infusion rate 0.325 \u03bcg/kg per minute [0.32-0.33]), and in four subjects (11%) in the postoperative 24 hours. No spinal haematoma, meningitis or epidural abscess were noted. Five subjects (14%) had a thrombopenia or a prothrombin time less than 60% before catheter removal. Two subjects (6%) had a leukopenia before catheter removal. No thrombopenia or blood coagulation disorders were recorded the day of catheter removal.\nEpidural analgesia for surgical treatment of peritoneal carcinomatosis: a risky technique?",
        "answer": "no",
        "answer_rationale": "In this series of 35 patients, the use of epidural analgesia for HIPEC does not seem to be associated with a worse risk of haemodynamic instability, spinal haematoma, meningitis or epidural abscess. HIPEC with platinum salt is not incompatible with the safety of epidural analgesia, with an optimized fluid management peroperatively and the following of perimedullary anesthesia practice guidelines.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "To study the risks of haemodynamic instability, and the possible occurrence of spinal haematoma, meningitis and epidural abscess when epidural analgesia is performed for cytoreductive surgery and hyperthermic intraperitoneal chemotherapy (HIPEC).\nWe retrospectively analyzed the data of 35 patients treated by HIPEC with oxaliplatin or cisplatin. An epidural catheter was inserted before induction of general anaesthesia. Postoperatively, a continuous epidural infusion of ropivacain, then a patient-controlled epidural analgesia were started.\nThe epidural catheter was used peroperatively before HIPEC in 12 subjects (34%), and after HIPEC in 23 subjects (66%). The median dose of ropivacain given peroperatively",
        "q2": " in the epidural catheter was 40 mg (30-75). Norepinephrin was used in two subjects (6%) peroperatively (median infusion rate 0.325 \u03bcg/kg per minute [0.32-0.33]), and in four subjects (11%) in the postoperative 24 hours. No spinal haematoma, meningitis or epidural abscess were noted. Five subjects (14%) had a thrombopenia or a prothrombin time less than 60% before catheter removal. Two subjects (6%) had a leukopenia before catheter removal. No thrombopenia or blood coagulation disorders were recorded the day of catheter removal.\nEpidural analgesia for surgical treatment of peritoneal carcinomatosis: a risky technique?",
        "generated_text": "was 0.3 mg/kg/h (0.1\u20130.6). The median duration of epidural analgesia was 3 days (1\u20136). The median dose of ropivacain given postoperatively was 0.2 mg/kg/h (0.1\u20130.3). The median dose of sufentanil given postoperatively was 0.1 \u03bcg/kg/h (0.1\u20130.2). The median dose of clonidine given postoperatively was 0.1 \u03bcg/kg/h (0.1\u20130.2). The median dose of ephedrine given peroperatively was 0.1 mg/kg/h (0.1\u20130.2). The median dose of ephedrine given postoperatively was 0.1 mg/kg",
        "levenshtein_distance": 481
    },
    {
        "realidx": "22188074",
        "question": "To investigate whether problems in instrumental activities of daily living (IADL) can add to conventionally used clinical measurements in helping to predict a diagnosis of dementia at 1- and 2-year follow-up.\nMulticenter prospective cohort study.\nMemory clinics in Europe.\nIndividuals aged 55 and older without dementia.\nIADLs were measured using pooled activities from five informant-based questionnaires. Structural equation modeling (SEM) was used to investigate the relation between IADLs and dementia. Age, sex, education, depression, and cognitive measures (Mini-Mental State Examination and verbal memory) were included in the model.\nFive hundred thirty-one participants had baseline and 1-year follow-up assessments; 69 (13.0%) of these had developed dementia at 1-year follow-up. At 2-year follow-up, 481 participants were seen, of whom 100 (20.8%) had developed dementia. Participants with IADL disabilities at baseline had a higher conversion rate (24.4%) than participants without IADL disabilities (16.7%) (chi-square\u00a0=\u00a04.28, degrees of freedom\u00a0=\u00a01, P\u00a0=\u00a0.04). SEM showed that IADL disability could help predict dementia in addition to the measured variables at 1-year follow-up (odds ratio (OR)\u00a0=\u00a02.20, 95% confidence interval (CI)\u00a0= 1.51-3.13) and 2-year follow-up (OR\u00a0=\u00a02.11, 95% CI\u00a0=\u00a01.33-3.33).\nDo instrumental activities of daily living predict dementia at 1- and 2-year follow-up?",
        "answer": "yes",
        "answer_rationale": "IADL disability is a useful addition to the diagnostic process in a memory clinic setting, indicating who is at higher risk of developing dementia at 1- and 2-year follow-up.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To investigate whether problems in instrumental activities of daily living (IADL) can add to conventionally used clinical measurements in helping to predict a diagnosis of dementia at 1- and 2-year follow-up.\nMulticenter prospective cohort study.\nMemory clinics in Europe.\nIndividuals aged 55 and older without dementia.\nIADLs were measured using pooled activities from five informant-based questionnaires. Structural equation modeling (SEM) was used to investigate the relation between IADLs and dementia. Age, sex, education, depression, and cognitive measures (Mini-Mental State Examination and verbal memory) were included in the model.\nFive hundred thirty-one participants had baseline and 1-year follow-up assessments; 69 (13.0%) of these had developed dementia at 1-year follow-up. At 2-year follow-up, 481 participants were seen, of",
        "q2": " whom 100 (20.8%) had developed dementia. Participants with IADL disabilities at baseline had a higher conversion rate (24.4%) than participants without IADL disabilities (16.7%) (chi-square\u00a0=\u00a04.28, degrees of freedom\u00a0=\u00a01, P\u00a0=\u00a0.04). SEM showed that IADL disability could help predict dementia in addition to the measured variables at 1-year follow-up (odds ratio (OR)\u00a0=\u00a02.20, 95% confidence interval (CI)\u00a0= 1.51-3.13) and 2-year follow-up (OR\u00a0=\u00a02.11, 95% CI\u00a0=\u00a01.33-3.33).\nDo instrumental activities of daily living predict dementia at 1- and 2-year follow-up?",
        "generated_text": "whom 103 (21.4%) had developed dementia. IADL impairment was a significant predictor of dementia at 1- and 2-year follow-up. The predictive value of IADL impairment was greater than that of age, sex, education, and depression and similar to that of cognitive measures. The predictive value of IADL impairment was independent of cognitive measures. The model showed good fit (comparative fit index = 0.98, root mean square error of approximation = 0.05).\nIADL impairment is an important risk factor for dementia in individuals without dementia. IADL impairment can be used to identify individuals at high risk of developing dementia in memory clinics.",
        "levenshtein_distance": 474
    },
    {
        "realidx": "22491528",
        "question": "To determine if composite measures based on process indicators are consistent with short-term outcome indicators in surgical colorectal cancer care.\nLongitudinal analysis of consistency between composite measures based on process indicators and outcome indicators for 85 Dutch hospitals.\nThe Dutch Surgical Colorectal Audit database, the Netherlands.\n4732 elective patients with colon carcinoma and 2239 with rectum carcinoma treated in 85 hospitals were included in the analyses.\nAll available process indicators were aggregated into five different composite measures. The association of the different composite measures with risk-adjusted postoperative mortality and morbidity was analysed at the patient and hospital level.\nAt the patient level, only one of the composite measures was negatively associated with morbidity for rectum carcinoma. At the hospital level, a strong negative association was found between composite measures and hospital mortality and morbidity rates for rectum carcinoma (p<0.05), and hospital morbidity rates for colon carcinoma.\nCombining process indicators to evaluate quality of care for surgical patients with colorectal cancer: are scores consistent with short-term outcome?",
        "answer": "maybe",
        "answer_rationale": "For individual patients, a high score on the composite measures based on process indicators is not associated with better short-term outcome. However, at the hospital level, a good score on the composite measures based on process indicators was consistent with more favourable risk-adjusted short-term outcome rates.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "To determine if composite measures based on process indicators are consistent with short-term outcome indicators in surgical colorectal cancer care.\nLongitudinal analysis of consistency between composite measures based on process indicators and outcome indicators for 85 Dutch hospitals.\nThe Dutch Surgical Colorectal Audit database, the Netherlands.\n4732 elective patients with colon carcinoma and 2239 with rectum carcinoma treated in 85 hospitals were included in the analyses.\nAll available process indicators were aggregated into five different composite measures. The association of the different composite measures",
        "q2": " with risk-adjusted postoperative mortality and morbidity was analysed at the patient and hospital level.\nAt the patient level, only one of the composite measures was negatively associated with morbidity for rectum carcinoma. At the hospital level, a strong negative association was found between composite measures and hospital mortality and morbidity rates for rectum carcinoma (p<0.05), and hospital morbidity rates for colon carcinoma.\nCombining process indicators to evaluate quality of care for surgical patients with colorectal cancer: are scores consistent with short-term outcome?",
        "generated_text": "with 30-day mortality, 30-day complication rate and 30-day reintervention rate was assessed using logistic regression analyses.\nThe composite measures based on process indicators were not associated with 30-day mortality, 30-day complication rate and 30-day reintervention rate. The composite measures were not consistent with the outcome indicators.\nComposite measures based on process indicators are not consistent with short-term outcome indicators in surgical colorectal cancer care. Therefore, composite measures based on process indicators should not be used as a",
        "levenshtein_distance": 424
    },
    {
        "realidx": "22537902",
        "question": "Synchronous liver metastases (SLM) occur in 20% of colorectal cancers (CRC). Resection of SLM and CLC can be undertaken at different centres (separate management, SM) or at the same centre (global management, GM).\nRetrospective study of SLM and CRC resections carried out during 01/2000 - 12/2006 by SM or GM, using a combined or delayed strategy.\nMorphologic characteristics and type of CRC and SLM resection were similar for the GM (n = 45) or SM (n = 66) groups. In patients with delayed liver resection (62 SM, 17 GM), chemotherapy prior to liver surgery was used in 92% and 38% of SM and GM patients (P<0.0001) and the median delay between procedures was 212 and 182 days, respectively (P = 0.04). First step of liver resection was more often performed during colorectal surgery in the GM group (62 vs. 6% for SM, P<0.0001) and the mean number of procedures (CRC+SLM) was lower (1.6 vs. 2.3, P = 0.003). Three-month mortality was 3% for GM and 0% for SM (n.s.). Overall survival rates were 67% and 51% for SM and GM at 3 years (n.s.), and 35 and 31% at 5 years (n.s.). Disease-free survival to 5 years was higher in SM patients (14% vs. 11%, P = 0.009).\nColorectal cancer with synchronous liver metastases: does global management at the same centre improve results?",
        "answer": "no",
        "answer_rationale": "GM of CRC and SLM was associated with fewer procedures but did not influence overall survival. SM was associated with a longer delay and increased use of chemotherapy between procedures, suggesting that more rigorous selection of SM patients for surgery may explain the higher disease-free survival after SLM resection.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Synchronous liver metastases (SLM) occur in 20% of colorectal cancers (CRC). Resection of SLM and CLC can be undertaken at different centres (separate management, SM) or at the same centre (global management, GM).\nRetrospective study of SLM and CRC resections carried out during 01/2000 - 12/2006 by SM or GM, using a combined or delayed strategy.\nMorphologic characteristics and type of CRC and SLM resection were similar for the GM (n = 45) or SM (n = 66) groups. In patients with delayed liver resection (62 SM, 17 GM), chemotherapy prior to liver surgery was used in 92% and 38% of SM and GM patients (P<0.0001) and the median delay between procedures was 212 and 182 days",
        "q2": ", respectively (P = 0.04). First step of liver resection was more often performed during colorectal surgery in the GM group (62 vs. 6% for SM, P<0.0001) and the mean number of procedures (CRC+SLM) was lower (1.6 vs. 2.3, P = 0.003). Three-month mortality was 3% for GM and 0% for SM (n.s.). Overall survival rates were 67% and 51% for SM and GM at 3 years (n.s.), and 35 and 31% at 5 years (n.s.). Disease-free survival to 5 years was higher in SM patients (14% vs. 11%, P = 0.009).\nColorectal cancer with synchronous liver metastases: does global management at the same centre improve results?",
        "generated_text": "(P = 0.02). In patients with combined resection (4 SM, 28 GM), the median delay between procedures was 0 and 1 day (P = 0.0001). The 5-year overall survival (OS) was 40% and 30% in the GM and SM groups (P = 0.3). In the GM group, OS was 50% and 30% in patients with combined and delayed resection (P = 0.1). In the SM group, OS was 0% and 30% in patients with combined and delayed resection (P = 0.1). In the GM group, the 5-year disease-free survival (DFS) was 20% and 10% in patients with combined and delayed resection (P = 0.1). In the SM group, DFS was 0% and",
        "levenshtein_distance": 461
    },
    {
        "realidx": "22694248",
        "question": "Although the retroperitoneal approach has been the preferred choice for open urological procedures, retroperitoneoscopy is not the preferred approach for laparoscopy. This study aims to develop a training model for retroperitoneoscopy and to establish an experimental learning curve.\nFifteen piglets were operated on to develop a standard retroperitoneoscopic nephrectomy (RPN) training model. All procedures were performed with three ports. Intraoperative data (side, operative time, blood loss, peritoneal opening) were recorded. Animals were divided into groups A, the first eight, and B, the last seven cases. Data were statistically analyzed.\nWe performed fifteen RPNs. The operative time varied from 15 to 50 minutes (median 30 minutes). Blood loss varied from 5 to 100 mL (median 20 mL). We experienced five peritoneal openings; we had two surgical vascular complications managed laparoscopically. There was statistical difference between groups A and B for peritoneal opening (p = 0.025), operative time (p = 0.0037), and blood loss (p = 0.026).\nRPN in a porcine model could simulate the whole procedure, from creating the space to nephrectomy completion. Experimental learning curve was eight cases, after statistical data analysis.\nIs there a model to teach and practice retroperitoneoscopic nephrectomy?",
        "answer": "yes",
        "answer_rationale": "RPN in a porcine model is feasible and could be very useful for teaching and practicing retroperitoneoscopy.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Although the retroperitoneal approach has been the preferred choice for open urological procedures, retroperitoneoscopy is not the preferred approach for laparoscopy. This study aims to develop a training model for retroperitoneoscopy and to establish an experimental learning curve.\nFifteen piglets were operated on to develop a standard retroperitoneoscopic nephrectomy (RPN) training model. All procedures were performed with three ports. Intraoperative data (side, operative time, blood loss, peritoneal opening) were recorded. Animals were divided into groups A, the first eight, and B, the last seven cases. Data were statistically analyzed.\nWe performed fifteen RPNs. The operative time varied from",
        "q2": " 15 to 50 minutes (median 30 minutes). Blood loss varied from 5 to 100 mL (median 20 mL). We experienced five peritoneal openings; we had two surgical vascular complications managed laparoscopically. There was statistical difference between groups A and B for peritoneal opening (p = 0.025), operative time (p = 0.0037), and blood loss (p = 0.026).\nRPN in a porcine model could simulate the whole procedure, from creating the space to nephrectomy completion. Experimental learning curve was eight cases, after statistical data analysis.\nIs there a model to teach and practice retroperitoneoscopic nephrectomy?",
        "generated_text": "20 to 90 minutes (mean 50.6 minutes). The mean blood loss was 20 mL. There were no major complications. The mean operative time in group A was 63.7 minutes and in group B 35.7 minutes (P = 0.001). The mean blood loss was 25 mL in group A and 14.2 mL in group B (P = 0.1). The peritoneum was opened in 50% of the cases in group A and in 14.2% in group B (P = 0.1).\nThe retroperitoneoscopic nephrectomy in the pig is a feasible and reproducible procedure. The experimental learning curve was achieved",
        "levenshtein_distance": 433
    },
    {
        "realidx": "23025584",
        "question": "That alcohol consumption is strongly influenced by the drinking behavior of social company has been demonstrated in observational research. However, not everyone is equally vulnerable to other people's drinking, and it is important to unravel which factors underlie these individual differences. This study focuses on the role of psychosocial stress in attempting to explain individual differences in the propensity to imitate alcohol consumption.\nWith a 2 (confederate's drinking condition: alcohol vs. soda) \u00d7 2 (participant's stress condition: stress vs. no stress) experimental design, we tested whether the tendency to imitate other people's drinking was related to participants' induced stress levels. The young male adults (N = 106) were randomly assigned to each of the conditions. In each session, directly after the stress or no-stress period, confederates and participants entered a bar laboratory where we observed their drinking behavior. Prior to entering the session, confederates were instructed to drink alcohol or soda.\nParticipants in both stress and no-stress conditions consumed substantially more alcohol when confederates drank alcohol than when they drank soda. There was no difference in alcohol consumed between stress and no-stress conditions. No moderating effect of stress on the tendency to drink along with peers was found.\nDoes stress increase imitation of drinking behavior?",
        "answer": "no",
        "answer_rationale": "Generally, it appears that among young male adults, imitation of alcohol consumption is a robust phenomenon not dependent on individual stress levels.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "That alcohol consumption is strongly influenced by the drinking behavior of social company has been demonstrated in observational research. However, not everyone is equally vulnerable to other people's drinking, and it is important to unravel which factors underlie these individual differences. This study focuses on the role of psychosocial stress in attempting to explain individual differences in the propensity to imitate alcohol consumption.\nWith a 2 (confederate's drinking condition: alcohol vs. soda) \u00d7 2 (participant's stress condition: stress vs. no stress) experimental design, we tested whether the tendency to imitate other people's drinking was related to participants' induced stress levels. The young male",
        "q2": " adults (N = 106) were randomly assigned to each of the conditions. In each session, directly after the stress or no-stress period, confederates and participants entered a bar laboratory where we observed their drinking behavior. Prior to entering the session, confederates were instructed to drink alcohol or soda.\nParticipants in both stress and no-stress conditions consumed substantially more alcohol when confederates drank alcohol than when they drank soda. There was no difference in alcohol consumed between stress and no-stress conditions. No moderating effect of stress on the tendency to drink along with peers was found.\nDoes stress increase imitation of drinking behavior?",
        "generated_text": "adults (N = 112) in this study were randomly assigned to 1 of the 4 conditions. After the stress induction, participants were exposed to a same-sex confederate who was instructed to consume either alcoholic or nonalcoholic beverages in a taste-rating task. Next, participants performed the same taste-rating task themselves, during which their alcohol consumption was unobtrusively measured.\nThe results showed that participants who were exposed to a drinking confederate consumed more alcohol than participants who were exposed to a nondrinking confederate. However, this effect was only present for participants in the stress condition. In the no",
        "levenshtein_distance": 479
    },
    {
        "realidx": "23283159",
        "question": "To investigate the effect of obesity at the start of adolescence on the prevalence, incidence and maintenance of chest wheezing among individuals aged 11-15 years in a birth cohort in a developing country.\nThe seventh follow-up of the 1993 Pelotas birth cohort occurred in 2004 (individuals aged 10-11 years). Between January and August 2008, the eighth follow-up of the cohort was conducted. All the individuals of the original cohort who were alive (who were then adolescents aged between 14 and 15 years) were targets for the study. The International Study of Asthma and Allergies in Childhood (ISAAC) questionnaire was used to define wheezing. In addition to the body mass index (BMI), used to define obesity by the World Health Organization (WHO) criteria, we assessed skinfold thickness.\nFrom the original cohort, 4,349 individuals were located (85.7% follow-up rate). The prevalence of chest wheezing at 11 and 15 years were 13.5% (95% CI: 12.5%-14.5%) and 12.1% (95% CI: 11.1%-13.1%), respectively. The prevalence of wheezing at both times was 4.5% (95% CI: 3.9%-5.1%) and the incidence of wheezing was 7.5% (95% CI: 6.7%-8.3%). Independent of the effect of various confounding variables, the prevalence of wheezing at 15 years was 50% greater among obese individuals than among eutrophic individuals at 11 years (RR 1.53; 95% CI: 1.14-2.05). The greater the skinfold tertile at 11 years, the higher the prevalence of wheezing at 15 years was (p = .011). Weight status and skinfolds did not present any association with incident wheezing. After controlling for confounding factors, the risk of persistent wheezing among obese individuals at 11 years was 1.82 (95% CI: 1.30-2.54).\nIs obesity a risk factor for wheezing among adolescents?",
        "answer": "yes",
        "answer_rationale": "Since obesity at the start of adolescence is associated with asthma symptom persistence, prevention and treatment of obesity may reduce avoidable healthcare costs and disease burden.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To investigate the effect of obesity at the start of adolescence on the prevalence, incidence and maintenance of chest wheezing among individuals aged 11-15 years in a birth cohort in a developing country.\nThe seventh follow-up of the 1993 Pelotas birth cohort occurred in 2004 (individuals aged 10-11 years). Between January and August 2008, the eighth follow-up of the cohort was conducted. All the individuals of the original cohort who were alive (who were then adolescents aged between 14 and 15 years) were targets for the study. The International Study of Asthma and Allergies in Childhood (ISAAC) questionnaire was used to define wheezing. In addition to the body mass index (BMI), used to define obesity by the World Health Organization (WHO) criteria, we assessed skinfold thickness.\nFrom the original cohort, 4,349 individuals were located (85.7% follow-up rate). The prevalence of chest wheezing at 11 and 15 years were 13.5% (95% CI: 12.5%-14.5%) and 12",
        "q2": ".1% (95% CI: 11.1%-13.1%), respectively. The prevalence of wheezing at both times was 4.5% (95% CI: 3.9%-5.1%) and the incidence of wheezing was 7.5% (95% CI: 6.7%-8.3%). Independent of the effect of various confounding variables, the prevalence of wheezing at 15 years was 50% greater among obese individuals than among eutrophic individuals at 11 years (RR 1.53; 95% CI: 1.14-2.05). The greater the skinfold tertile at 11 years, the higher the prevalence of wheezing at 15 years was (p = .011). Weight status and skinfolds did not present any association with incident wheezing. After controlling for confounding factors, the risk of persistent wheezing among obese individuals at 11 years was 1.82 (95% CI: 1.30-2.54).\nIs obesity a risk factor for wheezing among adolescents?",
        "generated_text": ".4% (95% CI: 11.4%-13.3%), respectively. The incidence of wheezing between 11 and 15 years was 7.5% (95% CI: 6.7%-8.3%), and the maintenance of wheezing was 36.4% (95% CI: 32.3%-40.5%). Obesity at 11 years was associated with the incidence of wheezing (RR: 1.50; 95% CI: 1.10-2.05) and with the maintenance of wheezing (RR: 1.50; 95% CI: 1.10-2.05). The results were similar when skinfold thickness was used as a measure of obesity.\nObesity at the start of adolescence was associated with the incidence and maintenance of wheezing in the following 4 years.",
        "levenshtein_distance": 469
    },
    {
        "realidx": "23347337",
        "question": "To provide equality of cancer care to rural patients, Townsville Cancer Centre administers intensive chemotherapy regimens to rural patients with node-positive breast and metastatic colorectal cancers at the same doses as urban patients. Side-effects were usually managed by rural general practitioners locally.AIM: The aim is to determine the safety of this practice by comparing the profile of serious adverse events and dose intensities between urban and rural patients at the Townsville Cancer Centre.\nA retrospective audit was conducted in patients with metastatic colorectal and node-positive breast cancers during a 24-month period. Fisher's exact test was used for analysis. Rurality was determined as per rural, remote and metropolitan classification.\nOf the 121 patients included, 70 and 51 patients had breast and colon cancers respectively. The urban versus rural patient split among all patients, breast and colorectal cancer subgroups was 68 versus 53, 43 versus 27 and 25 versus 26 respectively. A total of 421 cycles was given with dose intensity of>95% for breast cancer in both groups (P>0.05). Rate of febrile neutropenia was 9.3% versus 7.4% (P = 0.56). For XELOX, rate of diarrhoea was 20% versus 19% (P = 0.66) and rate of vomiting was 20% versus 11% (P = 0.11). Only two patients were transferred to Townsville for admission. No toxic death occurred in either group.\nIs intensive chemotherapy safe for rural cancer patients?",
        "answer": "yes",
        "answer_rationale": "It appears safe to administer intensive chemotherapy regimens at standard doses to rural patients without increased morbidity or mortality. Support for general practitioners through phone or videoconferencing may reduce the safety concerns.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To provide equality of cancer care to rural patients, Townsville Cancer Centre administers intensive chemotherapy regimens to rural patients with node-positive breast and metastatic colorectal cancers at the same doses as urban patients. Side-effects were usually managed by rural general practitioners locally.AIM: The aim is to determine the safety of this practice by comparing the profile of serious adverse events and dose intensities between urban and rural patients at the Townsville Cancer Centre.\nA retrospective audit was conducted in patients with metastatic colorectal and node-positive breast cancers during a 24-month period. Fisher's exact test was used for analysis. Rurality was determined as per rural, remote and metropolitan classification.\nOf the 121 patients included, 70 and 51 patients had breast and colon cancers respectively. The urban versus rural patient split among all",
        "q2": " patients, breast and colorectal cancer subgroups was 68 versus 53, 43 versus 27 and 25 versus 26 respectively. A total of 421 cycles was given with dose intensity of>95% for breast cancer in both groups (P>0.05). Rate of febrile neutropenia was 9.3% versus 7.4% (P = 0.56). For XELOX, rate of diarrhoea was 20% versus 19% (P = 0.66) and rate of vomiting was 20% versus 11% (P = 0.11). Only two patients were transferred to Townsville for admission. No toxic death occurred in either group.\nIs intensive chemotherapy safe for rural cancer patients?",
        "generated_text": "patients was 60% versus 40%, and among breast and colon cancer patients was 59% versus 41% and 61% versus 39% respectively. The incidence of serious adverse events was 6% versus 4% (P = 0.7) in urban versus rural patients respectively. The dose intensity was 0.9 versus 0.9 (P = 0.8) in urban versus rural patients respectively. The incidence of serious adverse events in breast cancer patients was 5% versus 0% (P = 0.3) in urban versus rural patients respectively. The dose intensity was 0.9 versus 0.9 (P = 0.8) in urban versus rural patients respectively. The incidence of serious adverse events",
        "levenshtein_distance": 407
    },
    {
        "realidx": "23690198",
        "question": "Social and cultural factors combined with little information may prevent the diffusion of epidural analgesia for pain relief during childbirth. The present study was launched contemporarily to the implementation of analgesia for labor in our Department in order to perform a 2 years audit on its use. The goal is to evaluate the epidural acceptance and penetration into hospital practice by women and care givers and safety and efficacy during childbirth.\nThis audit cycle measured epidural analgesia performance against 4 standards: (1) Implementation of epidural analgesia for labor to all patients; (2) Acceptance and good satisfaction level reported by patients and caregivers. (3) Effectiveness of labor analgesia; (4) No maternal or fetal side effects.\nDuring the audit period epidural analgesia increased from 15.5% of all labors in the first trimester of the study to 51% in the last trimester (p<0.005). Satisfaction levels reported by patients and care givers were good. A hierarchical clustering analysis identified two clusters based on VAS (Visual Analogue Scale) time course: in 226 patients (cluster 1) VAS decreased from 8.5\u00b11.4 before to 4.1\u00b11.3 after epidural analgesia; in 1002 patients (cluster 2) VAS decreased from 8.12\u00b11.7 before (NS vs cluster 1), to 0.76\u00b10.79 after (p<0.001 vs before and vs cluster 2 after). No other differences between clusters were observed.\nImplementation of epidural analgesia for labor: is the standard of effective analgesia reachable in all women?",
        "answer": "yes",
        "answer_rationale": "Present audit shows that the process of implementation of labor analgesia was quick, successful and safe, notwithstanding the identification of one cluster of women with suboptimal response to epidural analgesia that need to be further studies, overall pregnant womens'adhesion to labor analgesia was satisfactory.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Social and cultural factors combined with little information may prevent the diffusion of epidural analgesia for pain relief during childbirth. The present study was launched contemporarily to the implementation of analgesia for labor in our Department in order to perform a 2 years audit on its use. The goal is to evaluate the epidural acceptance and penetration into hospital practice by women and care givers and safety and efficacy during childbirth.\nThis audit cycle measured epidural analgesia performance against 4 standards: (1) Implementation of epidural analgesia for labor to all patients; (2) Acceptance and good satisfaction level reported by patients and caregivers. (3) Effectiveness of labor analgesia; (4) No maternal or fetal side effects.\nDuring the audit period epidural analgesia increased from 15.5% of all labors in the first trimester of the",
        "q2": " study to 51% in the last trimester (p<0.005). Satisfaction levels reported by patients and care givers were good. A hierarchical clustering analysis identified two clusters based on VAS (Visual Analogue Scale) time course: in 226 patients (cluster 1) VAS decreased from 8.5\u00b11.4 before to 4.1\u00b11.3 after epidural analgesia; in 1002 patients (cluster 2) VAS decreased from 8.12\u00b11.7 before (NS vs cluster 1), to 0.76\u00b10.79 after (p<0.001 vs before and vs cluster 2 after). No other differences between clusters were observed.\nImplementation of epidural analgesia for labor: is the standard of effective analgesia reachable in all women?",
        "generated_text": "study to 30.5% in the last trimester. The acceptance of epidural analgesia was 100% in the group of women who received it. The satisfaction level was high in 80% of women and in 90% of caregivers. The effectiveness of epidural analgesia was 100%. No maternal or fetal side effects were reported.\nThe audit cycle showed that epidural analgesia for labor was successfully implemented in our Department. The acceptance and satisfaction level was high in women and caregivers. The effectiveness of epidural analgesia was 100% and no maternal or fetal side effects were reported.",
        "levenshtein_distance": 460
    },
    {
        "realidx": "23949294",
        "question": "Recently, there has been increasing interest in the role of \"treatment as prevention\" (TasP). Some of the questions regarding TasP strategies arise from the perceived difficulties in achieving and maintaining viral load (VL) suppression over time and the risk of emergence of viral resistance that could compromise future treatment options. This study was conducted to assess these questions in a resource-limited setting.\nWe performed a retrospective observational study of HIV-infected patients diagnosed in the pre-HAART era on follow-up at a private center from Buenos Aires, Argentina. Socio-demographic, clinical, and laboratory data were extracted from clinical charts. Analyses were performed to test for potential associations of selected variables with current virologic failure or use of third-line drugs.\nOf 619 patients on follow-up, 82 (13.2%) were diagnosed in the pre-HAART era. At the time of our study, 79 (96.3%) patients were on HAART, with a median duration of 14 years (IQR 12-15) of therapy, and exposure to mono or dual nucleoside reverse transcriptase inhibitors regimens in 47.8% of cases.\u00a0Sixty-nine patients (87.3%) had undetectable VL, 37 (46.8%) never presented virologic failure, and 19 (24.1%) experienced only one failure. Thirteen patients (16.5%) were receiving third-line ART regimens, with an average of 2.7-fold more virologic failures than those on first- or second-line regimens (p = 0.007).\nTreatment as prevention in resource-limited settings: is it feasible to maintain HIV viral load suppression over time?",
        "answer": "yes",
        "answer_rationale": "Maintaining viral load suppression over time in resource-limited-settings is feasible.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Recently, there has been increasing interest in the role of \"treatment as prevention\" (TasP). Some of the questions regarding TasP strategies arise from the perceived difficulties in achieving and maintaining viral load (VL) suppression over time and the risk of emergence of viral resistance that could compromise future treatment options. This study was conducted to assess these questions in a resource-limited setting.\nWe performed a retrospective observational study of HIV-infected patients diagnosed in the pre-HAART era on follow-up at a private center from Buenos Aires, Argentina. Socio-demographic, clinical, and laboratory data were extracted from clinical charts. Analyses were performed to test for potential associations of selected variables with current virologic failure or use of third-line drugs.\nOf 619 patients on follow-up, 82 (13.2%) were diagnosed in the pre-HAART era.",
        "q2": " At the time of our study, 79 (96.3%) patients were on HAART, with a median duration of 14 years (IQR 12-15) of therapy, and exposure to mono or dual nucleoside reverse transcriptase inhibitors regimens in 47.8% of cases.\u00a0Sixty-nine patients (87.3%) had undetectable VL, 37 (46.8%) never presented virologic failure, and 19 (24.1%) experienced only one failure. Thirteen patients (16.5%) were receiving third-line ART regimens, with an average of 2.7-fold more virologic failures than those on first- or second-line regimens (p = 0.007).\nTreatment as prevention in resource-limited settings: is it feasible to maintain HIV viral load suppression over time?",
        "generated_text": "Median time since diagnosis was 20.5 years (IQR 19.0-22.0). Median time on HAART was 15.0 years (IQR 13.0-16.0). Median CD4+ T-cell count was 600 cells/mm3 (IQR 400-800). Seventy-nine patients (96.3%) were on HAART, 76 (96.2%) had VL <50 copies/mL, and 3 (3.8%) had detectable VL. Median time with VL <50 copies/mL was 5.0 years (IQR 2.0-8.0). No significant differences were found in the proportion of patients with VL <50 copies/mL when comparing patients on first-line (96.4%), second-line (96.0%), or third-line (100%) HAART (p",
        "levenshtein_distance": 490
    },
    {
        "realidx": "24014276",
        "question": "Studies examining predictors of survival among the oldest-old have primarily focused on objective measures, such as physical function and health status. Only a few studies have examined the effect of personality traits on survival, such as optimism. The aim of this study was to examine whether an optimistic outlook predicts survival among the oldest-old.\nThe Danish 1905 Cohort Survey is a nationwide, longitudinal survey comprising all individuals born in Denmark in 1905. At baseline in 1998, a total of 2,262 persons aged 92 or 93 agreed to participate in the intake survey. The baseline in-person interview consisted of a comprehensive questionnaire including physical functioning and health, and a question about whether the respondent had an optimistic, neutral or pessimistic outlook on his or her own future.\nDuring the follow-up period of 12 years (1998-2010) there were 2,239 deaths (99 %) in the 1905 Cohort Survey. Univariable analyses revealed that optimistic women and men were at lower risk of death compared to their neutral counterparts [HR 0.82, 95 % CI (0.73-0.93) and 0.81, 95 % CI (0.66-0.99), respectively]. When confounding factors such as baseline physical and cognitive functioning and disease were taken into account the association between optimism and survival weakened in both sexes, but the general pattern persisted. Optimistic women were still at lower risk of death compared to neutral women [HR 0.85, 95 % CI (0.74-0.97)]. The risk of death was also decreased for optimistic men compared to their neutral counterparts, but the effect was non-significant [HR 0.91, 95 % CI (0.73-1.13)].\nOptimism and survival: does an optimistic outlook predict better survival at advanced ages?",
        "answer": "yes",
        "answer_rationale": "An optimistic outlook appears to be a significant predictor of survival among the oldest-old women. It may also be a significant predictor for men but the sample size is small.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Studies examining predictors of survival among the oldest-old have primarily focused on objective measures, such as physical function and health status. Only a few studies have examined the effect of personality traits on survival, such as optimism. The aim of this study was to examine whether an optimistic outlook predicts survival among the oldest-old.\nThe Danish 1905 Cohort Survey is a nationwide, longitudinal survey comprising all individuals born in Denmark in 1905. At baseline in 1998, a total of 2,262 persons aged 92 or 93 agreed to participate in the intake survey. The baseline in-person interview consisted of a comprehensive questionnaire including physical functioning and health, and a question about whether the respondent had an optimistic, neutral or pessimistic outlook on his or her own future.\nDuring the follow-up period of 12 years (1998-2010) there were 2,239 deaths (99 %) in the 1905 Cohort Survey.",
        "q2": " Univariable analyses revealed that optimistic women and men were at lower risk of death compared to their neutral counterparts [HR 0.82, 95 % CI (0.73-0.93) and 0.81, 95 % CI (0.66-0.99), respectively]. When confounding factors such as baseline physical and cognitive functioning and disease were taken into account the association between optimism and survival weakened in both sexes, but the general pattern persisted. Optimistic women were still at lower risk of death compared to neutral women [HR 0.85, 95 % CI (0.74-0.97)]. The risk of death was also decreased for optimistic men compared to their neutral counterparts, but the effect was non-significant [HR 0.91, 95 % CI (0.73-1.13)].\nOptimism and survival: does an optimistic outlook predict better survival at advanced ages?",
        "generated_text": "The Cox proportional hazards model showed that the most optimistic individuals had a significantly lower mortality risk than the most pessimistic individuals (HR = 0.83, 95 % CI: 0.72-0.96) after adjustment for sex, education, marital status, smoking, physical functioning, cognitive functioning, and self-rated health. The association was slightly attenuated when further adjusted for depression (HR = 0.85, 95 % CI: 0.73-0.98).\nAn optimistic outlook on one\u2019s own future was associated with lower mortality risk among the oldest-old, even after adjustment for physical and cognitive functioning, self-rated health, and depression.",
        "levenshtein_distance": 560
    },
    {
        "realidx": "24245816",
        "question": "To compare the primary stability of miniscrews inserted into bone blocks of different bone mineral densities (BMDs) with and without cortical bone, and investigate whether some trabecular properties could influence primary stability.\nFifty-two bone blocks were extracted from fresh bovine pelvic bone. Four groups were created based on bone type (iliac or pubic region) and presence or absence of cortical bone. Specimens were micro-computed tomography imaged to evaluate trabecular thickness, trabecular number, trabecular separation, bone volume density (BV/TV), BMD, and cortical thickness. Miniscrews 1.4 mm in diameter and 6 mm long were inserted into the bone blocks, and primary stability was evaluated by insertion torque (IT), mini-implant mobility (PTV), and pull-out strength (PS).\nIntergroup comparison showed lower levels of primary stability when the BMD of trabecular bone was lower and in the absence of cortical bone (P\u2264.05). The Pearson correlation test showed correlation between trabecular number, trabecular thickness, BV/TV, trabecular BMD, total BMD, and IT, PTV, and PS. There was correlation between cortical thickness and IT and PS (P\u2264.05).\nIs trabecular bone related to primary stability of miniscrews?",
        "answer": "yes",
        "answer_rationale": "Cancellous bone plays an important role in primary stability of mini-implants in the presence or absence of cortical bone.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "To compare the primary stability of miniscrews inserted into bone blocks of different bone mineral densities (BMDs) with and without cortical bone, and investigate whether some trabecular properties could influence primary stability.\nFifty-two bone blocks were extracted from fresh bovine pelvic bone. Four groups were created based on bone type (iliac or pubic region) and presence or absence of cortical bone. Specimens were micro-computed tomography imaged to evaluate trabecular thickness, trabecular number, trabecular separation, bone volume density (BV/TV), BMD, and cortical thickness. Miniscrews 1.4 mm in diameter and 6 mm long",
        "q2": " were inserted into the bone blocks, and primary stability was evaluated by insertion torque (IT), mini-implant mobility (PTV), and pull-out strength (PS).\nIntergroup comparison showed lower levels of primary stability when the BMD of trabecular bone was lower and in the absence of cortical bone (P\u2264.05). The Pearson correlation test showed correlation between trabecular number, trabecular thickness, BV/TV, trabecular BMD, total BMD, and IT, PTV, and PS. There was correlation between cortical thickness and IT and PS (P\u2264.05).\nIs trabecular bone related to primary stability of miniscrews?",
        "generated_text": "were inserted into the bone blocks. Insertion torque (IT) and pullout strength (PS) were measured to evaluate primary stability. The correlations between trabecular properties and primary stability were analyzed.\nThe iliac bone group had significantly higher IT and PS than the pubic bone group (P < .05). The cortical bone group had significantly higher IT and PS than the cancellous bone group (P < .05). IT and PS were significantly correlated with trabecular thickness, trabecular number, trabecular separation, BV/TV, BMD, and cortical thickness (P < .05).\nPrimary stability was significantly higher in the iliac bone group than in",
        "levenshtein_distance": 452
    },
    {
        "realidx": "24476003",
        "question": "Although its excellent results, laparoscopic sleeve gastrectomy (LSG) presents major complications ranging from 0% to 29%. Among them, the staple line leak presents an incidence varying from 0% to 7%. Many trials debated about different solutions in order to reduce leaks' incidence. No author has investigated the role of gastric decompression in the prevention of this complication. Aim of our work is to evaluate if this procedure can play a role in avoiding the occurrence of staple line leaks after LSG.\nBetween January 2008 and November 2012, 145 patients were prospectively and randomly included in the study. Seventy patients composed the group A, whose operations were completed with placement of nasogastric tube; the other 75 patients were included in the group B, in which no nasogastric tube was placed.\nNo statistical differences were observed between group A and group B regarding gender distribution, age, weight, and BMI. No intraoperative complications and no conversion occurred in both groups. Intraoperative blood loss (50.1 \u00b1 42.3 vs. 52.5 \u00b1 37.6 ml, respectively) and operative time (65.4 \u00b1 25.5 vs. 62.6 \u00b1 27.8 min, respectively) were comparable between the two groups (p: NS). One staple line leak (1.4%) occurred on 6th postoperative day in group A patients. No leak was observed in group B patients. Postoperative hospital stay was significantly longer in group A vs. group B patients (7.6 \u00b1 3.4 vs. 6.2 \u00b1 3.1 days, respectively, p: 0.04).\nIs nasogastric decompression useful in prevention of leaks after laparoscopic sleeve gastrectomy?",
        "answer": "no",
        "answer_rationale": "Routine placement of nasogastric tube in patients operated of LSG seems not useful in reducing leaks' incidence.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "Although its excellent results, laparoscopic sleeve gastrectomy (LSG) presents major complications ranging from 0% to 29%. Among them, the staple line leak presents an incidence varying from 0% to 7%. Many trials debated about different solutions in order to reduce leaks' incidence. No author has investigated the role of gastric decompression in the prevention of this complication. Aim of our work is to evaluate if this procedure can play a role in avoiding the occurrence of staple line leaks after LSG.\nBetween January 2008 and November 2012, 145 patients were prospectively and randomly included in the study. Seventy patients composed the group A, whose operations were completed with placement of nasogastric tube; the other 75 patients were included in the group B, in which no nasogastric tube was placed.\nNo statistical differences were observed between group A and group B regarding gender",
        "q2": " distribution, age, weight, and BMI. No intraoperative complications and no conversion occurred in both groups. Intraoperative blood loss (50.1 \u00b1 42.3 vs. 52.5 \u00b1 37.6 ml, respectively) and operative time (65.4 \u00b1 25.5 vs. 62.6 \u00b1 27.8 min, respectively) were comparable between the two groups (p: NS). One staple line leak (1.4%) occurred on 6th postoperative day in group A patients. No leak was observed in group B patients. Postoperative hospital stay was significantly longer in group A vs. group B patients (7.6 \u00b1 3.4 vs. 6.2 \u00b1 3.1 days, respectively, p: 0.04).\nIs nasogastric decompression useful in prevention of leaks after laparoscopic sleeve gastrectomy?",
        "generated_text": ", age, preoperative body mass index, and comorbidities. No mortality was observed. In group A, 1 patient (1.4%) developed a leak, whereas in group B, 2 patients (2.6%) developed a leak. No statistical differences were observed between the 2 groups regarding the incidence of leaks (P=0.6). No statistical differences were observed between the 2 groups regarding the incidence of other complications.\nOur study shows that the placement of a nasogastric tube after LSG does not reduce the incidence of staple line leaks. Moreover, it does not reduce the incidence of other complications. Therefore, we believe that this procedure is not necessary in the postoperative management of patients undergoing LSG.",
        "levenshtein_distance": 545
    },
    {
        "realidx": "24481006",
        "question": "79 adjacent proximal surfaces without restorations in permanent teeth were examined. Patients suspected to have carious lesions after a visual clinical and a bitewing examination participated in a CBCT examination (Kodak 9000 3D, 5 \u00d7 3.7 cm field of view, voxel size 0.07 mm). Ethical approval and informed consent were obtained according to the Helsinki Declaration. Radiographic assessment recording lesions with or without cavitation was performed by two observers in bitewings and CBCT sections. Orthodontic separators were placed interdentally between two lesion-suspected surfaces. The separator was removed after 3 days and the surfaces recorded as cavitated (yes/no), i.e. validated clinically. Differences between the two radiographic modalities (sensitivity, specificity and overall accuracy) were estimated by analyzing the binary data in a generalized linear model.\nFor both observers, sensitivity was significantly higher for CBCT than for bitewings (average difference 33%, p<0.001) while specificity was not significantly different between the methods (p = 0.19). The overall accuracy was also significantly higher for CBCT (p<0.001).\nShould cavitation in proximal surfaces be reported in cone beam computed tomography examination?",
        "answer": "yes",
        "answer_rationale": "CBCT was more accurate in detecting cavitation in proximal surfaces than bitewing radiographs; therefore a CBCT examination performed for other clinical applications should also be assessed for proximal surface cavities in teeth without restorations, and when detected, this pathology must be part of the dentist's report.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "79 adjacent proximal surfaces without restorations in permanent teeth were examined. Patients suspected to have carious lesions after a visual clinical and a bitewing examination participated in a CBCT examination (Kodak 9000 3D, 5 \u00d7 3.7 cm field of view, voxel size 0.07 mm). Ethical approval and informed consent were obtained according to the Helsinki Declaration. Radiographic assessment recording lesions with or without cavitation was performed by two observers in bitewings and CBCT sections. Orthodontic separators were placed interdentally between two lesion-suspected surfaces. The separator was removed after ",
        "q2": "3 days and the surfaces recorded as cavitated (yes/no), i.e. validated clinically. Differences between the two radiographic modalities (sensitivity, specificity and overall accuracy) were estimated by analyzing the binary data in a generalized linear model.\nFor both observers, sensitivity was significantly higher for CBCT than for bitewings (average difference 33%, p<0.001) while specificity was not significantly different between the methods (p = 0.19). The overall accuracy was also significantly higher for CBCT (p<0.001).\nShould cavitation in proximal surfaces be reported in cone beam computed tomography examination?",
        "generated_text": "1 week and the surfaces were examined clinically for cavitation. The clinical examination was used as reference. Sensitivity, specificity, and accuracy were calculated for the radiographic methods. The interobserver agreement was calculated using Cohen's kappa.\n\nThe sensitivity, specificity, and accuracy for CBCT were 0.80, 0.88, and 0.85, respectively, and for bitewings 0.60, 0.88, and 0.77, respectively. The interobserver agreement was 0.80 for CBCT and 0.76 for bitewings.\n\nCBCT showed higher sensitivity",
        "levenshtein_distance": 437
    },
    {
        "realidx": "24516646",
        "question": "The ImmunoCAP ISAC 112 is a fluoro-immunoassay that allows detection of specific IgE to 112 molecular components from 51 allergenic sources. We studied the reliability of this technique intra- and inter- assay, as well as inter-batch- and inter-laboratory-assay.\nTwenty samples were studied, nineteen sera from polysensitized allergic patients, and the technique calibrator provided by the manufacturer (CTR02). We measured the sIgE from CTR02 and three patients' sera ten times in the same and in different assays. Furthermore, all samples were tested in two laboratories and with two batches of ISAC kit. To evaluate the accuracy of ISAC 112, we contrasted the determinations of CTR02 calibrator with their expected values by T Student test. To analyse the precision, we calculated the coefficient of variation (CV) of the 15 allergens that generate the calibration curve, and to analyse the repeatability and the reproducibility, we calculated the intraclass coefficient correlation (ICC) to each allergen.\nThe results obtained for CTR02 were similar to those expected in 7 of 15 allergens that generate the calibration curve, whereas in 8 allergens the results showed significant differences. The mean CV obtained in the CTR02 determinations was of 9.4%, and the variability of sera from patients was of 22.9%. The agreement in the intra- and inter-assay analysis was very good to 94 allergens and good to one. In the inter-batch analyse, we obtained a very good agreement to 82 allergens, good to 14, moderate to 5 allergens, poor to one, and bad to 1 allergen. In the inter-laboratory analyse, we obtained a very good agreement to 73 allergens, good to 22, moderate to 6 and poor to two allergens.\nIs the determination of specific IgE against components using ISAC 112 a reproducible technique?",
        "answer": "yes",
        "answer_rationale": "The allergen microarray immunoassay, ISAC 112, is a repeatable and reproducible in vitro diagnostic tool for determination of sIgE beyond the own laboratory.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "The ImmunoCAP ISAC 112 is a fluoro-immunoassay that allows detection of specific IgE to 112 molecular components from 51 allergenic sources. We studied the reliability of this technique intra- and inter- assay, as well as inter-batch- and inter-laboratory-assay.\nTwenty samples were studied, nineteen sera from polysensitized allergic patients, and the technique calibrator provided by the manufacturer (CTR02). We measured the sIgE from CTR02 and three patients' sera ten times in the same and in different assays. Furthermore, all samples were tested in two laboratories and with two batches of ISAC kit. To evaluate the accuracy of ISAC 112, we contrasted the determinations of CTR02 calibrator with their expected values by T Student test. To analyse the precision, we calculated the coefficient of variation (CV) of the 15 allergens that generate the calibration curve, and to analyse the repeatability and the reproducibility",
        "q2": ", we calculated the intraclass coefficient correlation (ICC) to each allergen.\nThe results obtained for CTR02 were similar to those expected in 7 of 15 allergens that generate the calibration curve, whereas in 8 allergens the results showed significant differences. The mean CV obtained in the CTR02 determinations was of 9.4%, and the variability of sera from patients was of 22.9%. The agreement in the intra- and inter-assay analysis was very good to 94 allergens and good to one. In the inter-batch analyse, we obtained a very good agreement to 82 allergens, good to 14, moderate to 5 allergens, poor to one, and bad to 1 allergen. In the inter-laboratory analyse, we obtained a very good agreement to 73 allergens, good to 22, moderate to 6 and poor to two allergens.\nIs the determination of specific IgE against components using ISAC 112 a reproducible technique?",
        "generated_text": ", we calculated the CV of the 112 allergens of the three patients' sera.\nThe accuracy of the ISAC 112 was good, since the expected values of the CTR02 calibrator were not significantly different from the observed values (P > 0.05). The precision of the ISAC 112 was also good, since the CV of the 15 allergens that generate the calibration curve was 5.3% intra-assay, 5.6% inter-assay, 5.7% inter-batch and 6.4% inter-laboratory. The repeatability and reproducibility of the ISAC 112 were also good, since the CV of the 112 allergens of the three patients' sera was 5.5% intra-assay, 5.8% inter-assay, 5.9% inter-batch and 6.5% inter-laboratory.\nThe ISAC 112 is a reliable technique for the detection of sIg",
        "levenshtein_distance": 611
    },
    {
        "realidx": "24698298",
        "question": "To assess pain intensity with and without subcutaneous local anesthesia prior to intraarticular administration of contrast medium for magnetic resonance arthrography (MRa) of the shoulder.\nThis single-center study was conducted after an IRB waiver of authorization, between January 2010 and December 2012. All patients provided written, informed consent for the procedure. Our prospectively populated institutional database was searched, based on our inclusion criteria. There were 249 outpatients (178 men and 71 women; mean age, 44.4 years \u00b1 14.6; range, 15-79) who underwent MRa and were enrolled in this study. Patients were excluded if they had received surgery of the shoulder before MRa, had undergone repeated MRa of the same shoulder, and/or had undergone MRa of both shoulders on the same day. Patients were randomly assigned into one of three groups. Patients in group A (n=61) received skin infiltration with local anesthesia. Patients in control group B (n=92) and group C (n=96) did not receive local anesthesia. Pain levels were immediately assessed after the injection for MRa using a horizontal visual analog scale (VAS) that ranged from 0 to 10. To compare the pain scores of the three groups for male and female patients, a two-way analysis of variance was used. A p-value equal to or less than 0.05 was considered to indicate a significant result.\nPatients who received local anesthesia (group A) showed a mean pain level on the VAS of 2.6 \u00b1 2.3. In patients who did not receive local anesthetics (groups B and C), a mean pain level on the VAS of 2.6 \u00b1 2.2 and 2.7 \u00b1 2.4 were detected, respectively. Between the three groups, no statistically significant difference in pain intensity was detected (p=.960). There were significant differences in subjective pain perception between men and women (p=.009). Moreover, the sex difference in all three groups was equal (p=.934).\nMR arthrography of the shoulder: do we need local anesthesia?",
        "answer": "no",
        "answer_rationale": "Local anesthesia is not required to lower a patient's pain intensity when applying intra-articular contrast media for MR arthrography of the shoulder. This could result in reduced costs and a reduced risk of adverse reactions, without an impact on patient comfort.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "To assess pain intensity with and without subcutaneous local anesthesia prior to intraarticular administration of contrast medium for magnetic resonance arthrography (MRa) of the shoulder.\nThis single-center study was conducted after an IRB waiver of authorization, between January 2010 and December 2012. All patients provided written, informed consent for the procedure. Our prospectively populated institutional database was searched, based on our inclusion criteria. There were 249 outpatients (178 men and 71 women; mean age, 44.4 years \u00b1 14.6; range, 15-79) who underwent MRa and were enrolled in this study. Patients were excluded if they had received surgery of the shoulder before MRa, had undergone repeated MRa of the same shoulder, and/or had undergone MRa of both shoulders on the same day. Patients were randomly assigned into one of three groups. Patients in group A (n=61) received skin infiltration with local anesthesia. Patients in control group B (n=92) and group C (n=96) did not receive local",
        "q2": " anesthesia. Pain levels were immediately assessed after the injection for MRa using a horizontal visual analog scale (VAS) that ranged from 0 to 10. To compare the pain scores of the three groups for male and female patients, a two-way analysis of variance was used. A p-value equal to or less than 0.05 was considered to indicate a significant result.\nPatients who received local anesthesia (group A) showed a mean pain level on the VAS of 2.6 \u00b1 2.3. In patients who did not receive local anesthetics (groups B and C), a mean pain level on the VAS of 2.6 \u00b1 2.2 and 2.7 \u00b1 2.4 were detected, respectively. Between the three groups, no statistically significant difference in pain intensity was detected (p=.960). There were significant differences in subjective pain perception between men and women (p=.009). Moreover, the sex difference in all three groups was equal (p=.934).\nMR arthrography of the shoulder: do we need local anesthesia?",
        "generated_text": "anesthesia. Pain intensity was assessed using a visual analog scale (VAS) immediately after the procedure. The Kruskal-Wallis test was used to compare the VAS scores between the three groups. The Mann-Whitney U test was used to compare the VAS scores between two groups. A p-value of <0.05 was considered statistically significant.\nThe mean VAS score was 2.3 \u00b1 1.6 in group A, 2.4 \u00b1 1.6 in group B, and 2.3 \u00b1 1.6 in group C. There was no significant difference in the VAS scores between the three groups (p=0.88). There was no significant difference in the VAS scores between group A and group B (p=0.80), group A and group C (p=0.99), and group B and group C (p=0.80).\nSubcutaneous local anesthesia prior to intraarticular administration of contrast medium for MRa of the shoulder does not reduce pain intensity.",
        "levenshtein_distance": 635
    },
    {
        "realidx": "25007420",
        "question": "Francophones may experience poorer health due to social status, cultural differences in lifestyle and attitudes, and language barriers to health care. Our study sought to compare mental health indicators between Francophones and non-Francophones living in the province of Manitoba.\nTwo populations were used: one from administrative datasets housed at the Manitoba Centre for Health Policy and the other from representative survey samples. The administrative datasets contained data from physician billings, hospitalizations, prescription drug use, education, and social services use, and surveys included indicators on language variables and on self-rated health.\nOutside urban areas, Francophones had lower rates of diagnosed substance use disorder (rate ratio [RR] = 0.80; 95% CI 0.68 to 0.95) and of suicide and suicide attempts (RR = 0.59; 95% CI 0.43 to 0.79), compared with non-Francophones, but no differences were found between the groups across the province in rates of diagnosed mood disorders, anxiety disorders, dementia, or any mental disorders after adjusting for age, sex, and geographic area. When surveyed, Francophones were less likely than non-Francophones to report that their mental health was excellent, very good, or good (66.9%, compared with 74.2%).\nAre there mental health differences between francophone and non-francophone populations in manitoba?",
        "answer": "maybe",
        "answer_rationale": "The discrepancy in how Francophones view their mental health and their rates of diagnosed mental disorders may be related to health seeking behaviours in the Francophone population. Community and government agencies should try to improve the mental health of this population through mental health promotion and by addressing language and cultural barriers to health services.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "Francophones may experience poorer health due to social status, cultural differences in lifestyle and attitudes, and language barriers to health care. Our study sought to compare mental health indicators between Francophones and non-Francophones living in the province of Manitoba.\nTwo populations were used: one from administrative datasets housed at the Manitoba Centre for Health Policy and the other from representative survey samples. The administrative datasets contained data from physician billings, hospitalizations, prescription drug use, education, and social services use, and surveys included indicators on language variables and on self-rated health.\nOutside urban areas, Francophones had lower rates of diagnosed substance use disorder (rate ratio [RR] = 0.80; 95% CI ",
        "q2": "0.68 to 0.95) and of suicide and suicide attempts (RR = 0.59; 95% CI 0.43 to 0.79), compared with non-Francophones, but no differences were found between the groups across the province in rates of diagnosed mood disorders, anxiety disorders, dementia, or any mental disorders after adjusting for age, sex, and geographic area. When surveyed, Francophones were less likely than non-Francophones to report that their mental health was excellent, very good, or good (66.9%, compared with 74.2%).\nAre there mental health differences between francophone and non-francophone populations in manitoba?",
        "generated_text": ": 0.66\u20130.97) and lower rates of diagnosed mood and anxiety disorders (RR = 0.88; 95% CI: 0.80\u20130.97) than non-Francophones. In urban areas, Francophones had higher rates of diagnosed mood and anxiety disorders (RR = 1.10; 95% CI: 1.01\u20131.20) than non-Francophones. Survey data showed that Francophones had higher rates of self-reported mood and anxiety disorders than non-Francophones (odds ratio = 1.30; 95% CI: 1.00\u20131.70).\nFrancophones in Manitoba have higher",
        "levenshtein_distance": 398
    },
    {
        "realidx": "25103647",
        "question": "To examine whether government-funded, low-income vision care programs improve use of eye care services by low-income individuals in Canada.\nCross-sectional survey.\n27,375 white respondents to the Canadian Community Health Survey (CCHS) Healthy Aging 2008/2009.\nGovernment-funded, low-income vision care programs were reviewed. The amount of assistance provided was compared with professional fee schedules for general/routine eye examinations and market prices for eyeglasses. The utilization of eye care providers was derived from the CCHS.\nTo receive low-income vision care assistance, individuals must be in receipt of social assistance. Criteria for receiving social assistance are stringent. The Canadian Financial Capability Survey revealed that 7.9% of Canadians aged 45 to 64 years and 5.5% aged \u226565 years received social assistance in 2009. The CCHS found in 2008/2009 that 12.5% of citizens aged 45 to 64 years and 13.2% of those aged \u226565 years had difficulty paying for basic expenses such as food. In 5 provinces, low-income vision care assistance fully covers a general/routine eye examination. In the remainder, the assistance provided is insufficient for a general/routine eye examination. The assistance for eyeglasses is inadequate in 5 provinces, requiring out-of-pocket copayments. Among middle-aged whites who self-reported not having glaucoma, cataracts, diabetes, or vision problems not corrected by lenses, utilization of eye care providers was 28.1% among those with financial difficulty versus 41.9% among those without (p<0.05), giving a prevalence ratio 0.68 (95% CI 0.57-0.80) adjusted for age, sex and education.\nDoes government assistance improve utilization of eye care services by low-income individuals?",
        "answer": "maybe",
        "answer_rationale": "Despite government assistance, low-income individuals use vision care services less often than wealthy individuals.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "To examine whether government-funded, low-income vision care programs improve use of eye care services by low-income individuals in Canada.\nCross-sectional survey.\n27,375 white respondents to the Canadian Community Health Survey (CCHS) Healthy Aging 2008/2009.\nGovernment-funded, low-income vision care programs were reviewed. The amount of assistance provided was compared with professional fee schedules for general/routine eye examinations and market prices for eyeglasses. The utilization of eye care providers was derived from the CCHS.\nTo receive low-income vision care assistance, individuals must be in receipt of social assistance. Criteria for receiving social assistance are stringent. The Canadian Financial Capability Survey revealed that 7.9% of Canadians aged 45 to 64 years and 5.5% aged \u226565 years received social assistance in 2009. The CCHS found in 2008/2009 that 12.5% of",
        "q2": " citizens aged 45 to 64 years and 13.2% of those aged \u226565 years had difficulty paying for basic expenses such as food. In 5 provinces, low-income vision care assistance fully covers a general/routine eye examination. In the remainder, the assistance provided is insufficient for a general/routine eye examination. The assistance for eyeglasses is inadequate in 5 provinces, requiring out-of-pocket copayments. Among middle-aged whites who self-reported not having glaucoma, cataracts, diabetes, or vision problems not corrected by lenses, utilization of eye care providers was 28.1% among those with financial difficulty versus 41.9% among those without (p<0.05), giving a prevalence ratio 0.68 (95% CI 0.57-0.80) adjusted for age, sex and education.\nDoes government assistance improve utilization of eye care services by low-income individuals?",
        "generated_text": "Canadians aged \u226545 years had not seen an eye care provider in the previous 2 years. The proportion of individuals who had not seen an eye care provider in the previous 2 years was 1.5 times higher among those with the lowest income (16.3%) than among those with the highest income (10.8%). The proportion of individuals who had not seen an eye care provider in the previous 2 years was 1.3 times higher among those with the lowest income who did not receive social assistance (17.0%) than among those with the lowest income who did receive social assistance (13.0%).\nGovernment-funded, low-income vision care programs in Canada do not appear to be effective in improving use of eye care services by low-income individuals.",
        "levenshtein_distance": 597
    },
    {
        "realidx": "25499207",
        "question": "Current evidence suggests that neck pain is negatively associated with health-related quality of life (HRQoL). However, these studies are cross-sectional and do not inform the association between neck pain and future HRQoL.\nThe purpose of this study was to investigate the association between increasing grades of neck pain severity and HRQoL 6 months later. In addition, this longitudinal study examines the crude association between the course of neck pain and HRQoL.\nThis is a population-based cohort study.\nEleven hundred randomly sampled Saskatchewan adults were included.\nOutcome measures were the mental component summary (MCS) and physical component summary (PCS) of the Short-Form-36 (SF-36) questionnaire.\nWe formed a cohort of 1,100 randomly sampled Saskatchewan adults in September 1995. We used the Chronic Pain Questionnaire to measure neck pain and its related disability. The SF-36 questionnaire was used to measure physical and mental HRQoL 6 months later. Multivariable linear regression was used to measure the association between graded neck pain and HRQoL while controlling for confounding. Analysis of variance and t tests were used to measure the crude association among four possible courses of neck pain and HRQoL at 6 months. The neck pain trajectories over 6 months were no or mild neck pain, improving neck pain, worsening neck pain, and persistent neck pain. Finally, analysis of variance was used to examine changes in baseline to 6-month PCS and MCS scores among the four neck pain trajectory groups.\nThe 6-month follow-up rate was 74.9%. We found an exposure-response relationship between neck pain and physical HRQoL after adjusting for age, education, arthritis, low back pain, and depressive symptomatology. Compared with participants without neck pain at baseline, those with mild (\u03b2=-1.53, 95% confidence interval [CI]=-2.83, -0.24), intense (\u03b2=-3.60, 95% CI=-5.76, -1.44), or disabling (\u03b2=-8.55, 95% CI=-11.68, -5.42) neck pain had worse physical HRQoL 6 months later. We did not find an association between neck pain and mental HRQoL. A worsening course of neck pain and persistent neck pain were associated with worse physical HRQoL.\nIs neck pain associated with worse health-related quality of life 6 months later?",
        "answer": "yes",
        "answer_rationale": "We found that neck pain was negatively associated with physical but not mental HRQoL. Our analysis suggests that neck pain may be a contributor of future poor physical HRQoL in the population. Raising awareness of the possible future impact of neck pain on physical HRQoL is important for health-care providers and policy makers with respect to the management of neck pain in populations.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Current evidence suggests that neck pain is negatively associated with health-related quality of life (HRQoL). However, these studies are cross-sectional and do not inform the association between neck pain and future HRQoL.\nThe purpose of this study was to investigate the association between increasing grades of neck pain severity and HRQoL 6 months later. In addition, this longitudinal study examines the crude association between the course of neck pain and HRQoL.\nThis is a population-based cohort study.\nEleven hundred randomly sampled Saskatchewan adults were included.\nOutcome measures were the mental component summary (MCS) and physical component summary (PCS) of the Short-Form-36 (SF-36) questionnaire.\nWe formed a cohort of 1,100 randomly sampled Saskatchewan adults in September 1995. We used the Chronic Pain Questionnaire to measure neck pain and its related disability. The SF-36 questionnaire was used to measure physical and mental HRQoL 6 months later. Multivariable linear regression was used to measure the association between graded neck pain and HRQoL while controlling for confounding. Analysis of variance and t tests were used to measure the crude association among four possible courses of neck pain and HRQoL at 6 months.",
        "q2": " The neck pain trajectories over 6 months were no or mild neck pain, improving neck pain, worsening neck pain, and persistent neck pain. Finally, analysis of variance was used to examine changes in baseline to 6-month PCS and MCS scores among the four neck pain trajectory groups.\nThe 6-month follow-up rate was 74.9%. We found an exposure-response relationship between neck pain and physical HRQoL after adjusting for age, education, arthritis, low back pain, and depressive symptomatology. Compared with participants without neck pain at baseline, those with mild (\u03b2=-1.53, 95% confidence interval [CI]=-2.83, -0.24), intense (\u03b2=-3.60, 95% CI=-5.76, -1.44), or disabling (\u03b2=-8.55, 95% CI=-11.68, -5.42) neck pain had worse physical HRQoL 6 months later. We did not find an association between neck pain and mental HRQoL. A worsening course of neck pain and persistent neck pain were associated with worse physical HRQoL.\nIs neck pain associated with worse health-related quality of life 6 months later?",
        "generated_text": "The response rate was 77%. After controlling for confounders, we found that increasing grades of neck pain were associated with clinically important reductions in PCS scores. The association between neck pain and MCS scores was not clinically important. The course of neck pain was associated with clinically important differences in PCS scores but not MCS scores.\nNeck pain is associated with clinically important reductions in future physical HRQoL. The course of neck pain is associated with clinically important differences in future physical HRQoL.",
        "levenshtein_distance": 712
    },
    {
        "realidx": "25588461",
        "question": "Disorders of consciousness (DOC) diagnosis relies on the presence or absence of purposeful motor responsiveness, which characterizes the minimally conscious state (MCS) and the unresponsive wakefulness syndrome (UWS), respectively. Functional neuroimaging studies have raised the question of possible residual conscious awareness also in clinically-defined UWS patients. The aim of our study was to identify electrophysiological parameters, by means of a transcranial magnetic stimulation approach, which might potentially express the presence of residual networks sustaining fragmentary behavioral patterns, even when no conscious behavior can be observed.\nWe enrolled 25 severe DOC patients, following post-anoxic or traumatic brain injury and 20 healthy individuals (HC) as control group. Baseline electrophysiological evaluation evidenced, in comparison to HC, a partial preservation of cortical effective connectivity and excitability in clinically defined MCS, whereas these components were absent in clinically defined UWS. Then, we applied an anodal transcranial direct current stimulation (a-tDCS) protocol over the orbitofrontal cortex.\na-tDCS was able to boost cortical connectivity and excitability in all HC, MCS, and to unmask such excitability/connectivity in some UWS patients.\nCan transcranial direct current stimulation be useful in differentiating unresponsive wakefulness syndrome from minimally conscious state patients?",
        "answer": "yes",
        "answer_rationale": "a-tDCS could be useful in identifying residual connectivity markers in clinically-defined UWS, who may lack of purposeful behavior as a result of a motor-output failure.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Disorders of consciousness (DOC) diagnosis relies on the presence or absence of purposeful motor responsiveness, which characterizes the minimally conscious state (MCS) and the unresponsive wakefulness syndrome (UWS), respectively. Functional neuroimaging studies have raised the question of possible residual conscious awareness also in clinically-defined UWS patients. The aim of our study was to identify electrophysiological parameters, by means of a transcranial magnetic stimulation approach, which might potentially express the presence of residual networks sustaining fragmentary behavioral patterns, even when no conscious behavior can be observed.\nWe enrolled 25 severe DOC patients, following post-anoxic or traumatic brain injury and 20 healthy",
        "q2": " individuals (HC) as control group. Baseline electrophysiological evaluation evidenced, in comparison to HC, a partial preservation of cortical effective connectivity and excitability in clinically defined MCS, whereas these components were absent in clinically defined UWS. Then, we applied an anodal transcranial direct current stimulation (a-tDCS) protocol over the orbitofrontal cortex.\na-tDCS was able to boost cortical connectivity and excitability in all HC, MCS, and to unmask such excitability/connectivity in some UWS patients.\nCan transcranial direct current stimulation be useful in differentiating unresponsive wakefulness syndrome from minimally conscious state patients?",
        "generated_text": "controls. We performed a single-pulse transcranial magnetic stimulation (TMS) coupled with high-density electroencephalography (hd-EEG) and we analyzed the TMS-evoked potentials (TEPs) and the TMS-evoked oscillations. We also computed the perturbational complexity index (PCI), a TMS/hd-EEG metric previously shown to differentiate conscious from unconscious states.\nWe found that the TEPs and the TMS-evoked oscillations were significantly different between MCS and UWS patients. In particular, the TEPs amplitude and the TMS-evoked oscillations power were higher in MCS than in UWS patients. Moreover,",
        "levenshtein_distance": 532
    },
    {
        "realidx": "25986020",
        "question": "Adoption and implementation of evidence-based measures for catheter care leads to reductions in central line-associated bloodstream infection (CLABSI) rates in the NICU. The purpose of this study is to evaluate whether this rate reduction is sustainable for at least 1 year and to identify key determinants of this sustainability at the NICU of the Floating Hospital for Children at Tufts Medical Center.\nWe reviewed the incidence of CLABSIs in the NICU temporally to the implementation of new practice policies and procedures, from July 2008 to December 2013.\nAdoption of standardized care practices, including bundles and checklists, was associated with a significant reduction of the CLABSI rate to zero for>370 consecutive days in our NICU in 2012. Overall, our CLABSI rates decreased from 4.1 per 1000 line days in 2009 (13 infections; 3163 line days) to 0.94 in 2013 (2 infections; 2115 line days), which represents a 77% reduction over a 5-year period. In the first quarter of 2013, there was a brief increase in CLABSI rate to 3.3 per 1000 line days; after a series of interventions, the CLABSI rate was maintained at zero for>600 days. Ongoing training, surveillance, and vigilance with catheter insertion and maintenance practices and improved documentation were identified as key drivers for success.\nIs zero central line-associated bloodstream infection rate sustainable?",
        "answer": "yes",
        "answer_rationale": "High-quality training, strict compliance with evidence-based guidelines, and thorough documentation is associated with significant reductions in CLABSIs. Mindful organizing may lead to a better understanding of what goes into a unit's ability to handle peak demands and sustain extraordinary performance in the long-term.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Adoption and implementation of evidence-based measures for catheter care leads to reductions in central line-associated bloodstream infection (CLABSI) rates in the NICU. The purpose of this study is to evaluate whether this rate reduction is sustainable for at least 1 year and to identify key determinants of this sustainability at the NICU of the Floating Hospital for Children at Tufts Medical Center.\nWe reviewed the incidence of CLABSIs in the NICU temporally to the implementation of new practice policies and procedures, from July 2008 to December 2013.\nAdoption of standardized care practices, including bundles and checklists, was associated with a significant reduction of the CLABSI rate to zero for>370 consecutive days in our NICU in 2012.",
        "q2": " Overall, our CLABSI rates decreased from 4.1 per 1000 line days in 2009 (13 infections; 3163 line days) to 0.94 in 2013 (2 infections; 2115 line days), which represents a 77% reduction over a 5-year period. In the first quarter of 2013, there was a brief increase in CLABSI rate to 3.3 per 1000 line days; after a series of interventions, the CLABSI rate was maintained at zero for>600 days. Ongoing training, surveillance, and vigilance with catheter insertion and maintenance practices and improved documentation were identified as key drivers for success.\nIs zero central line-associated bloodstream infection rate sustainable?",
        "generated_text": "This reduction was sustained for 1 year, with a CLABSI rate of 0.5 per 1000 catheter days in 2013. Key determinants of sustainability included the following: (1) a multidisciplinary team approach; (2) a culture of safety; (3) a focus on education and training; (4) a focus on communication; (5) a focus on standardization of care; and (6) a focus on data collection and feedback.\nImplementation of evidence-based measures for catheter care leads to significant reductions in CLABSI rates in the NICU. This reduction can be sustained for at least 1 year. Key determinants of sustainability include a multidisciplinary team approach, a culture of safety, a focus on education and training, a",
        "levenshtein_distance": 528
    },
    {
        "realidx": "26134053",
        "question": "Outcome feedback is the process of learning patient outcomes after their care within the emergency department. We conducted a national survey of Canadian Royal College emergency medicine (EM) residents and program directors to determine the extent to which active outcome feedback and follow-up occurred. We also compared the perceived educational value of outcome feedback between residents and program directors.\nWe distributed surveys to all Royal College-accredited adult and pediatric EM training programs using a modified Dillman method. We analyzed the data using student's t-test for continuous variables and Fisher's exact test for categorical variables.\nWe received 210 completed surveys from 260 eligible residents (80.8%) and 21 of 24 program directors (87.5%) (overall 81.3%). Mandatory active outcome feedback was not present in any EM training program for admitted or discharged patients (0/21). Follow-up was performed electively by 89.4% of residents for patients admitted to the hospital, and by 44.2% of residents for patients discharged home. A majority of residents (76.9%) believed that patient follow-up should be mandatory compared to 42.9% of program directors (p=0.002). The perceived educational value of outcome feedback was 5.8/7 for residents and 5.1/7 for program directors (difference 0.7; p=0.002) based on a seven-point Likert scale (1=not important; 7=very important).\nOutcome Feedback within Emergency Medicine Training Programs: An Opportunity to Apply the Theory of Deliberate Practice?",
        "answer": "maybe",
        "answer_rationale": "While Canadian EM training programs do not mandate follow-up, it is performed electively by the majority of residents surveyed. Residents place a significantly greater educational value on outcome feedback than their program directors, and believe that follow-up should be a mandatory component of EM residencies.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "C",
        "q1": "Outcome feedback is the process of learning patient outcomes after their care within the emergency department. We conducted a national survey of Canadian Royal College emergency medicine (EM) residents and program directors to determine the extent to which active outcome feedback and follow-up occurred. We also compared the perceived educational value of outcome feedback between residents and program directors.\nWe distributed surveys to all Royal College-accredited adult and pediatric EM training programs using a modified Dillman method. We analyzed the data using student's t-test for continuous variables and Fisher's exact test for categorical variables.\nWe received 210 completed surveys from 260 eligible residents (80.8%) and 21 of 24 program directors (87.5%) (overall 81.3%). Mandatory active outcome feedback was not present in any EM training program for",
        "q2": " admitted or discharged patients (0/21). Follow-up was performed electively by 89.4% of residents for patients admitted to the hospital, and by 44.2% of residents for patients discharged home. A majority of residents (76.9%) believed that patient follow-up should be mandatory compared to 42.9% of program directors (p=0.002). The perceived educational value of outcome feedback was 5.8/7 for residents and 5.1/7 for program directors (difference 0.7; p=0.002) based on a seven-point Likert scale (1=not important; 7=very important).\nOutcome Feedback within Emergency Medicine Training Programs: An Opportunity to Apply the Theory of Deliberate Practice?",
        "generated_text": "either adult or pediatric residents. Residents and program directors agreed that outcome feedback was educationally valuable (mean 4.1/5 and 4.3/5, respectively, p=0.2). Residents and program directors also agreed that outcome feedback was most valuable for critically ill patients (mean 4.3/5 and 4.4/5, respectively, p=0.6). Residents and program directors disagreed on the educational value of outcome feedback for patients with minor complaints (mean 2.6/5 and 3.3/5, respectively, p=0.01). Residents and program directors agreed that the most common barriers to outcome feedback were time constraints (mean 3.7/5 and 3.8/5, respectively,",
        "levenshtein_distance": 486
    },
    {
        "realidx": "26304701",
        "question": "The effect of preoperative education on anxiety and postoperative outcomes of cardiac surgery patients remains unclear.AIM: The aim of the study was to estimate the effectiveness of a nurse-led preoperative education on anxiety and postoperative outcomes.\nA randomised controlled study was designed. All the patients who were admitted for elective cardiac surgery in a general hospital in Athens with knowledge of the Greek language were eligible to take part in the study. Patients in the intervention group received preoperative education by specially trained nurses. The control group received the standard information by the ward personnel. Measurements of anxiety were conducted on admission-A, before surgery-B and before discharge-C by the state-trait anxiety inventory.\nThe sample consisted of 395 patients (intervention group: 205, control group: 190). The state anxiety on the day before surgery decreased only in the intervention group (34.0 (8.4) versus 36.9 (10.7); P=0.001). The mean decrease in state score during the follow-up period was greater in the intervention group (P=0.001). No significant difference was found in the length of stay or readmission. Lower proportions of chest infection were found in the intervention group (10 (5.3) versus 1 (0.5); P=0.004). Multivariate linear regression revealed that education and score in trait anxiety scale on admission are independent predictors of a reduction in state anxiety.\nCan nurse-led preoperative education reduce anxiety and postoperative complications of patients undergoing cardiac surgery?",
        "answer": "yes",
        "answer_rationale": "Preoperative education delivered by nurses reduced anxiety and postoperative complications of patients undergoing cardiac surgery, but it was not effective in reducing readmissions or length of stay.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "The effect of preoperative education on anxiety and postoperative outcomes of cardiac surgery patients remains unclear.AIM: The aim of the study was to estimate the effectiveness of a nurse-led preoperative education on anxiety and postoperative outcomes.\nA randomised controlled study was designed. All the patients who were admitted for elective cardiac surgery in a general hospital in Athens with knowledge of the Greek language were eligible to take part in the study. Patients in the intervention group received preoperative education by specially trained nurses. The control group received the standard information by the ward personnel. Measurements of anxiety were conducted on admission-A, before surgery-B and before discharge-C by the state-trait anxiety inventory.\nThe sample consisted of 395 patients (intervention group: 205, control group: 190",
        "q2": "). The state anxiety on the day before surgery decreased only in the intervention group (34.0 (8.4) versus 36.9 (10.7); P=0.001). The mean decrease in state score during the follow-up period was greater in the intervention group (P=0.001). No significant difference was found in the length of stay or readmission. Lower proportions of chest infection were found in the intervention group (10 (5.3) versus 1 (0.5); P=0.004). Multivariate linear regression revealed that education and score in trait anxiety scale on admission are independent predictors of a reduction in state anxiety.\nCan nurse-led preoperative education reduce anxiety and postoperative complications of patients undergoing cardiac surgery?",
        "generated_text": "). The mean age was 65.3 years (SD=10.7) and 80.3% were male. The two groups were similar in terms of demographic and clinical characteristics. The intervention group had significantly lower anxiety levels in all measurements (A: 40.1 vs. 44.1, p<0.001, B: 40.2 vs. 44.3, p<0.001, C: 35.1 vs. 37.5, p<0.001). The intervention group had significantly lower incidence of postoperative complications (p=0.001), shorter length of stay in the intensive care unit (p=0.001) and in the hospital (p=0.001).",
        "levenshtein_distance": 510
    },
    {
        "realidx": "27287237",
        "question": "Little is known about the validity and reliability of expert assessments of the quality of antimicrobial prescribing, despite their importance in antimicrobial stewardship. We investigated how infectious disease doctors' assessments compared with a reference standard (modal expert opinion) and with the assessments of their colleagues.\nTwenty-four doctors specialized in infectious diseases or clinical microbiology (16 specialists and 8 residents) from five hospitals were asked to assess the appropriateness of antimicrobial agents prescribed for a broad spectrum of indications in 56 paper cases. They were instructed how to handle guideline applicability and deviations. We created a reference standard of antimicrobial appropriateness using the modal assessment of 16 specialists. We calculated criterion validity and interrater and intrarater overall and specific agreement with an index expert (senior infectious disease physician) and analysed the influence of doctor characteristics on validity.\nSpecialists agreed with the reference standard in 80% of cases (range 75%-86%), with a sensitivity and specificity of 75% and 84%, respectively. This did not differ by clinical specialty, hospital or years of experience, and residents had similar results. Specialists agreed with the index expert in 76% of cases and the index expert agreed with his previous assessments in 71% of cases.\nAssessment of appropriate antimicrobial prescribing: do experts agree?",
        "answer": "yes",
        "answer_rationale": "Doctors specialized in infectious diseases and clinical microbiology assess the appropriateness of antimicrobials prescribed for a broad spectrum of indications with acceptable agreement and validity, regardless of their experience or hospital of employment. However, there is room for improvement, which merits attention in multidisciplinary discussions and education.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Little is known about the validity and reliability of expert assessments of the quality of antimicrobial prescribing, despite their importance in antimicrobial stewardship. We investigated how infectious disease doctors' assessments compared with a reference standard (modal expert opinion) and with the assessments of their colleagues.\nTwenty-four doctors specialized in infectious diseases or clinical microbiology (16 specialists and 8 residents) from five hospitals were asked to assess the appropriateness of antimicrobial agents prescribed for a broad spectrum of indications in 56 paper cases. They were instructed how to handle guideline applicability and deviations. We created a reference standard of antimicrobial appropriateness using the modal assessment of 16 specialists. We calculated",
        "q2": " criterion validity and interrater and intrarater overall and specific agreement with an index expert (senior infectious disease physician) and analysed the influence of doctor characteristics on validity.\nSpecialists agreed with the reference standard in 80% of cases (range 75%-86%), with a sensitivity and specificity of 75% and 84%, respectively. This did not differ by clinical specialty, hospital or years of experience, and residents had similar results. Specialists agreed with the index expert in 76% of cases and the index expert agreed with his previous assessments in 71% of cases.\nAssessment of appropriate antimicrobial prescribing: do experts agree?",
        "generated_text": "the percentage of agreement with the reference standard and the intraclass correlation coefficient (ICC) for inter-rater reliability.\nThe percentage of agreement with the reference standard was 80% (95% CI 78%\u201382%) for specialists and 75% (95% CI 72%\u201378%) for residents. The ICC for inter-rater reliability was 0.78 (95% CI 0.72\u20130.84) for specialists and 0.70 (95% CI 0.60\u20130.80) for residents. The ICC for inter-rater reliability was 0.76 (95% CI",
        "levenshtein_distance": 471
    },
    {
        "realidx": "27989969",
        "question": "The relationship between the use of an endoscope during ventriculoperitoneal shunt (VPS) procedures and infection remains poorly defined. In this study, we sought to analyze whether the simultaneous use of an endoscope could in fact increase the infection rate associated with VPS procedures.\nThis study included 438 VPS procedures, 49 in which an endoscope was used (11.2%) and 389 in which an endoscope was not used (88.8%). The infection rates in these 2 main groups were calculated and compared. Subsequently, 4 new groups were created, composed of patients with a shunt inserted for the first time (groups 1A and 1B) and patients with a shunt reviewed or inserted for a second time (groups 2A and 2B). Groups 1A and 2A comprised patients in whom an endoscope was used simultaneously with VPS surgery, and groups 1B and 2B comprised patients in whom an endoscope was not used. These groups were compared to determine the infection rate.\nThe overall infection rate was 18.5%, including 22.4% in the groups in which an endoscope was used and 18% in those in which an endoscope was not used (P\u00a0=\u00a00.449). Groups 1A and 1B and groups 2A and 2B were matched for possible intervening risk factors. The infection rate was 28.6% in group 1A and 16.2% in group 1B (P\u00a0= 0.27), and 20% in group 2A and 19.8% in group 2B (P\u00a0= 0.977).\nDoes the Simultaneous Use of a Neuroendoscope Influence the Incidence of Ventriculoperitoneal Shunt Infection?",
        "answer": "no",
        "answer_rationale": "In the present study, the use of an endoscope during VPS procedures did not increase the risk of surgical infection.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "The relationship between the use of an endoscope during ventriculoperitoneal shunt (VPS) procedures and infection remains poorly defined. In this study, we sought to analyze whether the simultaneous use of an endoscope could in fact increase the infection rate associated with VPS procedures.\nThis study included 438 VPS procedures, 49 in which an endoscope was used (11.2%) and 389 in which an endoscope was not used (88.8%). The infection rates in these 2 main groups were calculated and compared. Subsequently, 4 new groups were created, composed of patients with a shunt inserted for the first time (groups 1A and 1B) and patients with a shunt reviewed or inserted for a second time (groups 2A and 2B). Groups 1A and 2A comprised patients in whom an endoscope was used simultaneously with VPS surgery, and groups 1B",
        "q2": " and 2B comprised patients in whom an endoscope was not used. These groups were compared to determine the infection rate.\nThe overall infection rate was 18.5%, including 22.4% in the groups in which an endoscope was used and 18% in those in which an endoscope was not used (P\u00a0=\u00a00.449). Groups 1A and 1B and groups 2A and 2B were matched for possible intervening risk factors. The infection rate was 28.6% in group 1A and 16.2% in group 1B (P\u00a0= 0.27), and 20% in group 2A and 19.8% in group 2B (P\u00a0= 0.977).\nDoes the Simultaneous Use of a Neuroendoscope Influence the Incidence of Ventriculoperitoneal Shunt Infection?",
        "generated_text": "and 2B comprised patients in whom an endoscope was not used. The infection rates in these 4 groups were calculated and compared.\nThe infection rate in the group in which an endoscope was used was 6.1%, and the infection rate in the group in which an endoscope was not used was 3.1%. The difference was not statistically significant (P = 0.28). The infection rates in groups 1A, 1B, 2A, and 2B were 0%, 2.3%, 7.1%, and 3.6%, respectively. The differences between these groups were not statistically significant (P = 0.45).\nThe simultaneous use of an endoscope during VPS procedures does not increase the infection rate.",
        "levenshtein_distance": 380
    },
    {
        "realidx": "28707539",
        "question": "Measurement of visceral adipose tissue (VAT) needs to be accurate and sensitive to change for risk monitoring. The purpose of this study is to determine the CT slice location where VAT area can best reflect changes in VAT volume and body weight.\n60 plain abdominal CT images from 30 males\u00a0[mean age (range) 51 (41-68) years, mean body weight (range) 71.1 (101.9-50.9) kg] who underwent workplace screenings twice within a 1-year interval were evaluated. Automatically calculated and manually corrected areas of the VAT of various scan levels using \"freeform curve\" region of interest on CT were recorded and compared with body weight changes.\nThe strongest correlations of VAT area with VAT volume and body weight changes were shown in a slice 3 cm above the lower margin of L3 with r values of 0.853 and 0.902, respectively.\nVisceral adipose tissue area measurement at a single level: can it represent visceral adipose tissue volume?",
        "answer": "yes",
        "answer_rationale": "VAT area measurement at a single level 3 cm above the lower margin of the L3 vertebra is feasible and can reflect changes in VAT volume and body weight. Advances in knowledge: As VAT area at a CT slice 3cm above the lower margin of L3 can best reflect interval changes in VAT volume and body weight, VAT area measurement should be selected at this location.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "Measurement of visceral adipose tissue (VAT) needs to be accurate and sensitive to change for risk monitoring. The purpose of this study is to determine the CT slice location where VAT area can best reflect changes in VAT volume and body weight.\n60 plain abdominal CT images from 30 males\u00a0[mean age (range) 51 (41-68) years, mean body weight (range) 71.1 (101.9-50.9) kg] who underwent workplace screenings twice within a 1",
        "q2": "-year interval were evaluated. Automatically calculated and manually corrected areas of the VAT of various scan levels using \"freeform curve\" region of interest on CT were recorded and compared with body weight changes.\nThe strongest correlations of VAT area with VAT volume and body weight changes were shown in a slice 3 cm above the lower margin of L3 with r values of 0.853 and 0.902, respectively.\nVisceral adipose tissue area measurement at a single level: can it represent visceral adipose tissue volume?",
        "generated_text": "-year interval were analyzed. VAT areas were measured at the level of the umbilicus (L3/4) and the level of the lower endplate of the L4 vertebra (L4/5). VAT volume was measured from the diaphragm to the pelvis. The correlation between the change in VAT volume and the change in VAT area at each slice location was analyzed. The correlation between the change in body weight and the change in VAT area at each slice location was also analyzed.\nThe mean interval between the two",
        "levenshtein_distance": 362
    },
    {
        "realidx": "8521557",
        "question": "The insertion/deletion (ID) polymorphism of the angiotensin-converting enzyme (ACE) gene has been associated with increased coronary heart disease (CHD), although the mechanism of this association is not apparent. We tested the hypothesis that the deletion allele of the ACE gene is associated with insulin resistance.\nWe related ACE genotype to components of the insulin-resistance syndrome in 103 non-insulin-dependent diabetic (NIDDM) and 533 nondiabetic white subjects. NIDDM subjects with the DD genotype had significantly lower levels of specific insulin (DD 38.6, ID 57.1, and II 87.4 pmol.L-1 by ANOVA, P = .011). Non-insulin-treated subjects with the DD genotype had increased insulin sensitivity by HOMA % (DD 56.4%, II 29.4%, P = .027) and lower levels of des 31,32 proinsulin (DD 3.3, II 7.6 pmol.L-1, P = .012) compared with II subjects. There were no differences in prevalence of CHD or levels of blood pressure, serum lipids, or plasminogen activator inhibitor-1 (PAI-1) activity between the three ACE genotypes. In nondiabetic subjects there were no differences in insulin sensitivity, levels of insulin-like molecules, blood pressure, PAI-1, serum lipids, or CHD prevalence between the three ACE genotypes.\nThe insertion allele of the ACE gene I/D polymorphism. A candidate gene for insulin resistance?",
        "answer": "no",
        "answer_rationale": "We conclude that increased cardiovascular risk of the DD genotype is not mediated through insulin resistance or abnormalities in fibrinolysis. Conversely, we report an increased sensitivity in NIDDM subjects with the ACE DD genotype.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "B",
        "q1": "The insertion/deletion (ID) polymorphism of the angiotensin-converting enzyme (ACE) gene has been associated with increased coronary heart disease (CHD), although the mechanism of this association is not apparent. We tested the hypothesis that the deletion allele of the ACE gene is associated with insulin resistance.\nWe related ACE genotype to components of the insulin-resistance syndrome in 103 non-insulin-dependent diabetic (NIDDM) and 533 nondiabetic white subjects. NIDDM subjects with the DD genotype had significantly lower levels of specific insulin (DD 38.6, ID 57.1, and II 87.4 pmol.L-1 by ANOVA, P = .011). Non-insulin-treated subjects with the DD genotype had increased insulin sensitivity by HOMA %",
        "q2": " (DD 56.4%, II 29.4%, P = .027) and lower levels of des 31,32 proinsulin (DD 3.3, II 7.6 pmol.L-1, P = .012) compared with II subjects. There were no differences in prevalence of CHD or levels of blood pressure, serum lipids, or plasminogen activator inhibitor-1 (PAI-1) activity between the three ACE genotypes. In nondiabetic subjects there were no differences in insulin sensitivity, levels of insulin-like molecules, blood pressure, PAI-1, serum lipids, or CHD prevalence between the three ACE genotypes.\nThe insertion allele of the ACE gene I/D polymorphism. A candidate gene for insulin resistance?",
        "generated_text": "S (DD 60.8, ID 50.0, and II 50.0, P = .048). Nondiabetic subjects with the DD genotype had significantly lower levels of fasting insulin (DD 38.6, ID 45.0, and II 50.0 pmol.L-1, P = .001) and proinsulin (DD 2.0, ID 2.3, and II 2.6 pmol.L-1, P = .001). The DD genotype was also associated with lower systolic blood pressure (DD 126, ID 130, and II 131 mmHg, P = .001) and diastolic blood pressure (DD 75, ID 78, and II 78 mmHg, P = .001).",
        "levenshtein_distance": 454
    },
    {
        "realidx": "9465206",
        "question": "We investigated the actual role of MRI versus arthroscopy in the detection and characterization of occult bone and/or cartilage injuries in patients with previous musculoskeletal trauma of the knee, pain and severe functional impairment. Occult post-traumatic osteochondral injuries of the knee are trauma-related bone and/or cartilage damage missed at plain radiography.\nWe retrospectively selected 70 patients (men:women = 7:3; age range: 35 +/- 7 years) with a history of acute musculoskeletal trauma, negative conventional radiographs, pain and limited joint movements. All patients were submitted to conventional radiography, arthroscopy and MRI, the latter with 0.5 T units and T1-weighted SE. T2-weighted GE and FIR sequences with fat suppression.\nWe identified three types of occult post-traumatic injuries by morpho-topographic and signal intensity patterns: bone bruises (no. 25), subchondral (no. 33) and osteochondral (no. 35) injuries. Arthroscopy depicted 45 osteochondral and 19 chondral injuries. A bone bruise was defined as a typical subcortical area of signal loss, with various shapes, on T1-weighted images and of increased signal intensity on T2-weighted and FIR images. The cortical bone and articular cartilage were normal in all cases, while osteochondral injuries exhibited associated bone and cartilage damage with the same abnormal MR signal intensity. Sprain was the mechanism of injury in 52 cases, bruise in 12 and stress in 6. In 52 sprains (30 in valgus), the injury site was the lateral compartment in 92.3% of cases (100% in valgus), associated with meniscal damage in 73% of cases (90% in valgus) and with ligament injury in 90.4% (100% in valgus). In 12 bruises, the injury site was the lateral compartment in 58.3% of cases, the knee cap in 25% and the medial compartment in 16.7%; meniscal damage was associated in 25% of cases and ligament damage in 8.3%. In 6 stress injuries, the injury site was localized in the medial tibial condyle in 80% of cases, while meniscal and ligament tears were absent.\n\"Occult\" posttraumatic lesions of the knee: can magnetic resonance substitute for diagnostic arthroscopy?",
        "answer": "yes",
        "answer_rationale": "After comparing MR with arthroscopic findings and reviewing the available literature, we conclude that arthroscopy permits the direct visualization of even fine articular surface changes but does not depict the subchondral bone, the most frequent site of injury detected with MRI. MRI was a very useful tool in the detection and characterization of the different types of occult bone and/or cartilage injuries and showed a statistically significant correlation between site and distribution of bone and cartilage injuries and between internal damage and trauma mechanisms. Therefore, we believe that MRI can help avoid diagnostic arthroscopy in the patients with a history of post-traumatic pain, acute articular blocks and negative radiographic findings.",
        "options": {
            "A": "yes",
            "B": "no",
            "C": "maybe"
        },
        "answer_idx": "A",
        "q1": "We investigated the actual role of MRI versus arthroscopy in the detection and characterization of occult bone and/or cartilage injuries in patients with previous musculoskeletal trauma of the knee, pain and severe functional impairment. Occult post-traumatic osteochondral injuries of the knee are trauma-related bone and/or cartilage damage missed at plain radiography.\nWe retrospectively selected 70 patients (men:women = 7:3; age range: 35 +/- 7 years) with a history of acute musculoskeletal trauma, negative conventional radiographs, pain and limited joint movements. All patients were submitted to conventional radiography, arthroscopy and MRI, the latter with 0.5 T units and T1-weighted SE. T2-weighted GE and FIR sequences with fat suppression.\nWe identified three types of occult post-traumatic injuries by morpho-topographic and signal intensity patterns: bone bruises (no. 25), subchondral (no. 33) and osteochondral (no. 35) injuries. Arthroscopy depicted 45 osteochondral and 19 chondral injuries. A bone bruise was defined as a typical subcortical area of signal loss, with various shapes, on T1-weighted images and",
        "q2": " of increased signal intensity on T2-weighted and FIR images. The cortical bone and articular cartilage were normal in all cases, while osteochondral injuries exhibited associated bone and cartilage damage with the same abnormal MR signal intensity. Sprain was the mechanism of injury in 52 cases, bruise in 12 and stress in 6. In 52 sprains (30 in valgus), the injury site was the lateral compartment in 92.3% of cases (100% in valgus), associated with meniscal damage in 73% of cases (90% in valgus) and with ligament injury in 90.4% (100% in valgus). In 12 bruises, the injury site was the lateral compartment in 58.3% of cases, the knee cap in 25% and the medial compartment in 16.7%; meniscal damage was associated in 25% of cases and ligament damage in 8.3%. In 6 stress injuries, the injury site was localized in the medial tibial condyle in 80% of cases, while meniscal and ligament tears were absent.\n\"Occult\" posttraumatic lesions of the knee: can magnetic resonance substitute for diagnostic arthroscopy?",
        "generated_text": "hyperintensity on T2-weighted images. Subchondral injuries were defined as a subcortical area of signal loss on T1-weighted images and hyperintensity on T2-weighted images, with a linear pattern and a well-defined margin. Osteochondral injuries were defined as a subcortical area of signal loss on T1-weighted images and hyperintensity on T2-weighted images, with a linear pattern and a well-defined margin, associated with a chondral defect. MRI detected 93 osteochondral injuries, 25 bone bruises and 33 subchondral injuries. Arthroscopy detected 45 osteochondral and 19 chondral injuries. The sensitivity, specificity and accuracy of MRI were 100%, 100% and 100%, respectively, for bone bruises, 100%, 100% and 100% for subchondral injuries, and 100%, 100% and 100% for osteochondral injuries. The sensitivity, specificity and accuracy of arthroscopy were 0%, 0% and 0%, respectively, for bone bruises, 0%, 0% and 0% for subchondral injuries, and 100%, 100% and 100% for osteochond",
        "levenshtein_distance": 777
    }
]